{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d9d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sda1048/Desktop/interp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "import hashlib\n",
    "import yaml \n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2dd02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m baseline_model_hf = AutoModelForCausalLM.from_pretrained(baseline_model_path, torch_dtype=torch.bfloat16)\n\u001b[32m      7\u001b[39m baseline_model_tokenizer = AutoTokenizer.from_pretrained(baseline_model_path)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m baseline_model = \u001b[43mHookedTransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained_no_processing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_model_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_model_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmove_to_device\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1406\u001b[39m, in \u001b[36mHookedTransformer.from_pretrained_no_processing\u001b[39m\u001b[34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, fold_value_biases, dtype, default_prepend_bos, default_padding_side, **from_pretrained_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained_no_processing\u001b[39m(\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1399\u001b[39m     **from_pretrained_kwargs,\n\u001b[32m   1400\u001b[39m ):\n\u001b[32m   1401\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for from_pretrained.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \n\u001b[32m   1403\u001b[39m \u001b[33;03m    Wrapper for from_pretrained with all boolean flags related to simplifying the model set to\u001b[39;00m\n\u001b[32m   1404\u001b[39m \u001b[33;03m    False. Refer to from_pretrained for details.\u001b[39;00m\n\u001b[32m   1405\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_padding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_padding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1416\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1371\u001b[39m, in \u001b[36mHookedTransformer.from_pretrained\u001b[39m\u001b[34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[39m\n\u001b[32m   1363\u001b[39m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[32m   1364\u001b[39m model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m   1365\u001b[39m     cfg,\n\u001b[32m   1366\u001b[39m     tokenizer,\n\u001b[32m   1367\u001b[39m     move_to_device=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1368\u001b[39m     default_padding_side=default_padding_side,\n\u001b[32m   1369\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_process_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_writing_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter_unembed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_value_biases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefactor_factored_attn_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[32m   1381\u001b[39m     model.move_model_modules_to_device()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1638\u001b[39m, in \u001b[36mHookedTransformer.load_and_process_state_dict\u001b[39m\u001b[34m(self, state_dict, fold_ln, center_writing_weights, center_unembed, fold_value_biases, refactor_factored_attn_matrices)\u001b[39m\n\u001b[32m   1636\u001b[39m state_dict_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict_keys:\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1639\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m state_dict[key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2554\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2555\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2557\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2558\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2559\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2532\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2531\u001b[39m     local_metadata[\u001b[33m\"\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m\"\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2532\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2542\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2438\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   2436\u001b[39m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[32m   2437\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2438\u001b[39m             \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   2440\u001b[39m     action = \u001b[33m\"\u001b[39m\u001b[33mswapping\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcopying\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "\n",
    "reference_model_path = 'meta-llama/Llama-3.1-8B'\n",
    "baseline_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "baseline_model_hf = AutoModelForCausalLM.from_pretrained(baseline_model_path, torch_dtype=torch.bfloat16)\n",
    "baseline_model_tokenizer = AutoTokenizer.from_pretrained(baseline_model_path)\n",
    "\n",
    "baseline_model = HookedTransformer.from_pretrained_no_processing(\n",
    "    reference_model_path,\n",
    "    hf_model=baseline_model_hf,\n",
    "    tokenizer=baseline_model_tokenizer,\n",
    "    device=device,\n",
    "    move_to_device=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc84631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f887a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_feature_mode_metric(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    pos_features: list[list[int]],\n",
    "    neg_features: list[list[int]],\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - normalized probabilities for each feature sequence (sum to 1)\n",
    "      - unnormalized log-scores for each\n",
    "      - logit (log-odds) of producing any negative feature.\n",
    "    Uses model.to_tokens() for tokenization.\n",
    "    \"\"\"\n",
    "    device = model.cfg.device\n",
    "    # 1) Tokenize prompt with the built-in hook\n",
    "    #    (adds BOS if the model is configured to)\n",
    "    input_ids = model.to_tokens(prompt).to(device)\n",
    "\n",
    "    # 2) Score one feature sequence by accumulating log-probs\n",
    "    def sequence_score(feature: list[int]) -> torch.Tensor:\n",
    "        ctx = input_ids.clone()\n",
    "        total_log_prob = torch.tensor(0.0, device=device)\n",
    "        for tok in feature:\n",
    "            # run the model on the current context\n",
    "            logits, cache = model.run_with_cache(ctx)\n",
    "            last_logits = logits[:, -1, :]  # [1, vocab_size]\n",
    "            log_probs = torch.log_softmax(last_logits, dim=-1)\n",
    "            total_log_prob = total_log_prob + log_probs[0, tok]\n",
    "            # append the ground-truth token to the context\n",
    "            ctx = torch.cat([ctx, torch.tensor([[tok]], device=device)], dim=1)\n",
    "        return total_log_prob\n",
    "\n",
    "    # 3) Compute scores for all features\n",
    "    all_features = pos_features + neg_features\n",
    "    scores = torch.stack([sequence_score(f) for f in all_features])  # (n+m,)\n",
    "\n",
    "    # 4) Softmax to get normalized probabilities\n",
    "    norm_probs = torch.softmax(scores, dim=0)                        # (n+m,)\n",
    "\n",
    "    # 5) Sum up the negative-feature mass & compute logit\n",
    "    num_pos   = len(pos_features)\n",
    "    neg_prob  = norm_probs[num_pos:].sum()\n",
    "    neg_logit = torch.log(neg_prob) - torch.log(1 - neg_prob)\n",
    "\n",
    "    return norm_probs, scores, neg_logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b36ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "for pf in pos_features:\n",
    "    print(model.to_string(pf))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for nf in neg_features:\n",
    "    print(model.to_string(nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt       = \"<｜User｜>If a pizza is cut into 8 equal slices and 3 slices are eaten, what fraction remains?<｜Assistant｜><think>\\n\"\n",
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "norm_probs, scores, neg_logit = compute_feature_mode_metric(\n",
    "    model, prompt, pos_features, neg_features\n",
    ")\n",
    "\n",
    "print(\"Normalized probabilities:\", norm_probs)\n",
    "print(\"Unnormalized log-scores:\", scores)\n",
    "print(\"Negative-feature logit:\", neg_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ab916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt       = \"<｜User｜>If a pizza is cut into 8 equal slices and 3 slices are eaten, what fraction remains?<｜Assistant｜><think>\\nTo determine the fraction of the pizza that remains after eating 3 slices out of 8, I start by noting that the total number of slices is 8.\\n\\nNext, I calculate the fraction of the pizza that has been eaten by dividing the number of eaten slices by the total number of slices, which gives me 3/8.\\n\\nFinally, to find the fraction that remains, I subtract the eaten fraction from the whole, resulting in 1 minus 3/8, which equals 5/8.\\n</think>\\n\\n\"\n",
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "norm_probs, scores, neg_logit = compute_feature_mode_metric(\n",
    "    model, prompt, pos_features, neg_features\n",
    ")\n",
    "\n",
    "print(\"Normalized probabilities:\", norm_probs)\n",
    "print(\"Unnormalized log-scores:\", scores)\n",
    "print(\"Negative-feature logit:\", neg_logit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
