{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment that I want to do involves the most simple identification of facts within content-free tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from nnsight import CONFIG\n",
    "from nnsight import LanguageModel\n",
    "import nnsight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing from my own code \n",
    "from activation_transplanting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the api_key\n",
    "CONFIG.set_default_api_key(os.environ.get('NDIF_KEY'))\n",
    "\n",
    "# read the hf token\n",
    "os.environ['HF_TOKEN'] = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDIF_models = [\n",
    "    \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    \"meta-llama/Meta-Llama-3.1-70B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "] \n",
    "\n",
    "# inexaustive list\n",
    "non_NDIF_models = [\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts\n",
    "\n",
    "# instruct examples\n",
    "prompt_example_1 = \"<|begin▁of▁sentence|>\\n\" \\\n",
    "         \"<|start_header_id|>user<|end_header_id|>\\n\\n\" \\\n",
    "         \"Hello, how are you? <|eot_id|>\\n\" \\\n",
    "         \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "prompt_example_2 = \"<|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|>\\n\" \\\n",
    "                \"<|start_header_id|>user<|end_header_id|>\\n\\n\" \\\n",
    "                \"Answer the following in one word: What is the tallest mountain in the world?<|eot_id|>\\n\" \\\n",
    "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "# Base model examples \n",
    "prompt_example_3 = \"\\nUser: What's the capital of France?\\n\\nAssistant:\"\n",
    "\n",
    "# Reasoning examples \n",
    "prompt_example_4 = \"<｜User｜>Robert has three apples, and then gets one more. How many apples does he have? Respond in a single word.<｜Assistant｜>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers Experiment 1\n",
    "\n",
    "We'll be simply trying to identify the presence of stored numbers at particular tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_simple_number_string(num, mode='base'):\n",
    "    prefix = \"Word Problem Setup:\"\n",
    "    \n",
    "    # Define possible components for the problem\n",
    "    intros = [\"A man has \", \"A boy has \", \"Steven has \", \"Robert has \", \n",
    "             \"A woman has \", \"A girl has \", \"Sarah has \", \"Emily has \",\n",
    "             \"Alex has \", \"Jordan has \", \"Taylor has \", \"Sam has \"]\n",
    "    \n",
    "    numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \n",
    "              \"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n",
    "    \n",
    "    objects = [(\"apple\", \"apples\"), (\"banana\", \"bananas\"), (\"orange\", \"oranges\"),\n",
    "              (\"peach\", \"peaches\"), (\"pear\", \"pears\"), (\"grape\", \"grapes\"),\n",
    "              (\"strawberry\", \"strawberries\"), (\"blueberry\", \"blueberries\"),\n",
    "              (\"mango\", \"mangoes\"), (\"kiwi\", \"kiwis\"), (\"plum\", \"plums\")]\n",
    "    \n",
    "    suffixes = [\n",
    "        \"when he leaves the store\", \"when he leaves the shop\", \n",
    "        \"when he leaves the grocery store\", \"when he leaves the market\",\n",
    "        \"when she leaves the store\", \"when she leaves the shop\",\n",
    "        \"when she leaves the grocery store\", \"when she leaves the market\",\n",
    "        \"after shopping\", \"after grocery shopping\", \"after visiting the supermarket\"\n",
    "    ]\n",
    "    \n",
    "    ending = \".\\n\\n\"\n",
    "    \n",
    "    # Choose random components\n",
    "    intro = random.choice(intros)\n",
    "    \n",
    "    # Select appropriate number word and object form based on num\n",
    "    if 1 <= num <= 10:\n",
    "        number_word = numbers[num-1]\n",
    "        obj = random.choice(objects)\n",
    "        # Use singular or plural form based on num\n",
    "        object_word = obj[0] if num == 1 else obj[1]\n",
    "    else:\n",
    "        # For numbers > 10, just use the numeric form\n",
    "        number_word = str(num)\n",
    "        obj = random.choice(objects)\n",
    "        object_word = obj[1]  # Always use plural\n",
    "    \n",
    "    suffix = random.choice(suffixes)\n",
    "    \n",
    "    # Make pronoun in suffix match the intro person's implied gender\n",
    "    if (\"man\" in intro or \"boy\" in intro or \"Steven\" in intro or \"Robert\" in intro) and \"she\" in suffix:\n",
    "        suffix = suffix.replace(\" she \", \" he \")\n",
    "    elif (\"woman\" in intro or \"girl\" in intro or \"Sarah\" in intro or \"Emily\" in intro) and \"he\" in suffix:\n",
    "        suffix = suffix.replace(\" he \", \" she \")\n",
    "    \n",
    "    # Assemble the full problem\n",
    "    \n",
    "    if mode=='base':\n",
    "        problem = f\"{prefix} {intro}{number_word} {object_word} {suffix}{ending}\"\n",
    "    elif mode == 'instruct':\n",
    "        problem = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{prefix} {intro}{number_word} {object_word} {suffix}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    elif mode == 'reason':\n",
    "        problem = f\"<｜User｜>{prefix} {intro}{number_word} {object_word} {suffix}<｜Assistant｜>\"\n",
    "    else:\n",
    "        assert False, f\"mode {mode} not in [base, instruct, reason]\"\n",
    "    return problem, obj[1]\n",
    "\n",
    "\n",
    "def extract_final_logits(\n",
    "        tk,\n",
    "        source_strings: list[str],\n",
    "        target_strings: list[str],\n",
    "        target_substring: str,\n",
    "        occurrence_index: int = 0,\n",
    "        num_prev: int = 0,\n",
    "        num_fut: int = 0,\n",
    "        transplant_strings: tuple[str] = (\"residual\"),\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        extract the logits produced at the final position of target_strings\n",
    "        \"\"\"\n",
    "        assert num_prev >= 0\n",
    "        assert num_fut >= 0\n",
    "\n",
    "        # Extract newline activations from source strings\n",
    "        activation_containers, source_newline_indices = (\n",
    "            tk.extract_newline_activations(\n",
    "                strings=source_strings,\n",
    "                target_substring=target_substring,\n",
    "                occurrence_index=occurrence_index,\n",
    "                transplant_strings=transplant_strings,\n",
    "                num_prev=num_prev,\n",
    "                num_fut=num_fut,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(\"source_newline_indices\", source_newline_indices)\n",
    "        output_logits = []\n",
    "\n",
    "        # Process each target string with corresponding source activations\n",
    "        for target_string, activation_container, source_newline_index in zip(\n",
    "            target_strings, activation_containers, source_newline_indices\n",
    "        ):\n",
    "            print(\"source_newline_index\", source_newline_index)\n",
    "            act = activation_container.get_token_by_index(\n",
    "                source_newline_index\n",
    "            )\n",
    "            print(act)\n",
    "            \n",
    "            print(vars(activation_container))\n",
    "\n",
    "            final_logits = tk.evaluate_with_transplanted_activity(\n",
    "                target_string=target_string,\n",
    "                target_substring=target_substring,\n",
    "                activation_container=activation_container,\n",
    "                source_token_index=source_newline_index,\n",
    "                occurrence_index=occurrence_index,\n",
    "                transplant_strings=transplant_strings,\n",
    "                num_prev=num_prev,\n",
    "                num_fut=num_fut,\n",
    "            )\n",
    "            output_logits.append(final_logits)\n",
    "        \n",
    "        return output_logits\n",
    "\n",
    "def predict_number_probs(logits, llama):\n",
    "    logit_values = []\n",
    "    # add a prefix t get the token in context\n",
    "    prefix = \".\\n\\nThey have\"\n",
    "    numbers = [' one', ' two', ' three', ' four', ' five', ' six', ' seven', ' eight', ' nine', ' ten']\n",
    "    \n",
    "    for i, n in enumerate(numbers):\n",
    "        idx = llama.tokenizer.encode(prefix+n)[-1]\n",
    "        assert idx is not None\n",
    "        logit_values.append(logits[idx].float())\n",
    "    \n",
    "    all_probs = torch.nn.functional.softmax(torch.tensor(logit_values), dim=0)\n",
    "\n",
    "    # Sum probabilities for word forms and digit forms\n",
    "    return all_probs#[:10]+all_probs[10:]\n",
    "\n",
    "def evaluate_number_probs(strings, items, tk, target_substring=\".\\n\\n\"):\n",
    "    \"\"\" \n",
    "    for each string, we'll evaluate the probabilities of numbers\n",
    "    \"\"\"\n",
    "    question_strings = [f\"{target_substring} Therefore, the total number of {item} purchased was \" for item, s in zip(items, strings)]\n",
    "\n",
    "    for s, q in zip(strings, question_strings):\n",
    "        print(\"Source: \", s, \"\\nQuestion:\",q)\n",
    "\n",
    "    final_logits = extract_final_logits(\n",
    "        tk,\n",
    "        source_strings=strings,\n",
    "        target_strings=question_strings,\n",
    "        target_substring=target_substring,\n",
    "        occurrence_index= -1,\n",
    "        num_prev = 0,\n",
    "        num_fut = 0,\n",
    "        transplant_strings= (\"residual\",),\n",
    "    )\n",
    "    \n",
    "    extracted_probs = []\n",
    "    for logits in final_logits:\n",
    "        extracted_probs.append(predict_number_probs(logits, tk.llama))\n",
    "    \n",
    "    return extracted_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_number_experiment(tk, num, number_samples, target_substring=\".\\n\\n\", mode='base'):\n",
    "    \"\"\" \n",
    "    Choose a single num to use to generate sentences with \n",
    "    then generate number_samples with it\n",
    "\n",
    "    for each run evaluate_number_probs(strings, tk, target_substring)\n",
    "    to see the probability distribution over next numbers\n",
    "    \"\"\"\n",
    "    string_samples, items = zip(*[\n",
    "        generate_random_simple_number_string(num, mode=mode) for _ in range(number_samples)\n",
    "    ])\n",
    "\n",
    "    extracted_probs = evaluate_number_probs(string_samples, items, tk, target_substring=target_substring)\n",
    "\n",
    "    # now average over each \n",
    "    tot = 0\n",
    "    out=None\n",
    "    for p in extracted_probs:\n",
    "        if out is None:\n",
    "            out=p \n",
    "        else:\n",
    "            out+=p\n",
    "        \n",
    "        tot+=1\n",
    "    \n",
    "    return out.numpy()/tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# choose a model \\nllama_model_string = \"meta-llama/Meta-Llama-3.1-8B\"\\n# remote = use NDIF\\nremote = True \\n\\nif remote and (llama_model_string not in NDIF_models):\\n    remote = False \\n    print(\"Model not available on NDIF\")\\n\\n# load a model\\nllama = LanguageModel(llama_model_string)\\n\\n# commented out for now\\ntk = LLamaExamineToolkit(\\n    llama_model=llama, \\n    remote=True, # use NDIF\\n)\\nout = tk.transplant_newline_activities(\\n    source_strings=[\"\\nUser: What country is the Colosseum in?\\n\\nAssistant: \",]*5,\\n    target_strings=[\"\\nUser: What country is the Louvre in?\\n\\nAssistant: \",]*5,\\n    target_substring=\\'in?\\',\\n    num_new_tokens=100,\\n    occurrence_index=0,\\n    num_prev=0,\\n    num_fut=0,\\n    transplant_strings=(\\'residual\\',\\'key\\', \\'value\\', \\'output\\')\\n)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# choose a model \n",
    "llama_model_string = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "out = tk.transplant_newline_activities(\n",
    "    source_strings=[\"\\nUser: What country is the Colosseum in?\\n\\nAssistant: \",]*5,\n",
    "    target_strings=[\"\\nUser: What country is the Louvre in?\\n\\nAssistant: \",]*5,\n",
    "    target_substring='in?',\n",
    "    num_new_tokens=100,\n",
    "    occurrence_index=0,\n",
    "    num_prev=0,\n",
    "    num_fut=0,\n",
    "    transplant_strings=('residual','key', 'value', 'output')\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:12:41,334 6770bbff-ee84-4f56-b104-ed648731e98b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:12:42,188 6770bbff-ee84-4f56-b104-ed648731e98b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:12:42,772 6770bbff-ee84-4f56-b104-ed648731e98b - RUNNING: Your job has started running.\n",
      "2025-03-16 11:13:05,482 6770bbff-ee84-4f56-b104-ed648731e98b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 199M/199M [00:22<00:00, 9.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:13:34,846 65f16e85-4516-4e03-9531-592822cd8ba3 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:13:36,780 65f16e85-4516-4e03-9531-592822cd8ba3 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:13:38,581 65f16e85-4516-4e03-9531-592822cd8ba3 - RUNNING: Your job has started running.\n",
      "2025-03-16 11:13:44,083 65f16e85-4516-4e03-9531-592822cd8ba3 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 2.01k/2.01k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:13:50,201 a8fe6464-1214-451a-a56f-4cf94467dba2 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:13:51,312 a8fe6464-1214-451a-a56f-4cf94467dba2 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:13:53,343 a8fe6464-1214-451a-a56f-4cf94467dba2 - RUNNING: Your job has started running.\n",
      "2025-03-16 11:13:58,504 a8fe6464-1214-451a-a56f-4cf94467dba2 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 2.01k/2.01k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:14:03,359 f38e5bb9-d019-4f54-9347-867af027b490 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:14:08,344 f38e5bb9-d019-4f54-9347-867af027b490 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:14:08,344 f38e5bb9-d019-4f54-9347-867af027b490 - RUNNING: Your job has started running.\n",
      "2025-03-16 11:14:13,130 f38e5bb9-d019-4f54-9347-867af027b490 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 2.01k/2.01k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:14:17,965 583121da-ce29-4826-82eb-951af205639d - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:14:19,382 583121da-ce29-4826-82eb-951af205639d - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:14:20,950 583121da-ce29-4826-82eb-951af205639d - RUNNING: Your job has started running.\n",
      "2025-03-16 11:14:24,685 583121da-ce29-4826-82eb-951af205639d - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.76k/1.76k [00:00<00:00, 1.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n",
      "key\n",
      "value\n",
      "output\n",
      "residual\n",
      "source_token =  11 ('\\n\\n', 271)\n",
      "target_token =  1 ('\\n\\n',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 11:14:31,362 b467d488-4b14-4e08-99e5-f150f70320a4 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-16 11:14:33,040 b467d488-4b14-4e08-99e5-f150f70320a4 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-16 11:14:34,780 b467d488-4b14-4e08-99e5-f150f70320a4 - RUNNING: Your job has started running.\n",
      "2025-03-16 11:14:37,098 b467d488-4b14-4e08-99e5-f150f70320a4 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.24k/1.24k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# choose a model \n",
    "llama_model_string = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "out_base = tk.transplant_newline_activities(\n",
    "    source_strings=[\"\\nUser: Explain fractals in 150 words\\n\\n\",]*5,\n",
    "    target_strings=['\\n\\n',]*5,\n",
    "    target_substring='\\n\\n',\n",
    "    num_new_tokens=100,\n",
    "    occurrence_index=0,\n",
    "    num_prev=0,\n",
    "    num_fut=0,\n",
    "    transplant_strings=('residual','key', 'value', 'output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# choose a model \\nllama_model_string = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\\n# remote = use NDIF\\nremote = True \\n\\nif remote and (llama_model_string not in NDIF_models):\\n    remote = False \\n    print(\"Model not available on NDIF\")\\n\\n# load a model\\nllama = LanguageModel(llama_model_string)\\n\\n\\n\\n# commented out for now\\ntk = LLamaExamineToolkit(\\n    llama_model=llama, \\n    remote=True, # use NDIF\\n)\\nout_instruct = tk.transplant_newline_activities(\\n    source_strings=[\"<|start_header_id|>user<|end_header_id|>\\n\\n: Explain fractals in 150 words<|eot_id|>\",]*5,\\n    target_strings=[\\'<|eot_id|>\\',]*5,\\n    target_substring=\\'<|eot_id|>\\',\\n    num_new_tokens=100,\\n    occurrence_index=0,\\n    num_prev=0,\\n    num_fut=0,\\n    transplant_strings=(\\'residual\\',\\'key\\', \\'value\\', \\'output\\')\\n)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# choose a model \n",
    "llama_model_string = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "out_instruct = tk.transplant_newline_activities(\n",
    "    source_strings=[\"<|start_header_id|>user<|end_header_id|>\\n\\n: Explain fractals in 150 words<|eot_id|>\",]*5,\n",
    "    target_strings=['<|eot_id|>',]*5,\n",
    "    target_substring='<|eot_id|>',\n",
    "    num_new_tokens=100,\n",
    "    occurrence_index=0,\n",
    "    num_prev=0,\n",
    "    num_fut=0,\n",
    "    transplant_strings=('residual','key', 'value', 'output')\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting token activations\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target string '\n\n' occurrence 0 not found in string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# commented out for now\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tk \u001b[38;5;241m=\u001b[39m LLamaExamineToolkit(\n\u001b[0;32m     15\u001b[0m     llama_model\u001b[38;5;241m=\u001b[39mllama, \n\u001b[0;32m     16\u001b[0m     remote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# use NDIF\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m out_reason \u001b[38;5;241m=\u001b[39m \u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransplant_newline_activities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<｜User｜>Explain fractals in 150 words<｜Assistant｜>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_substring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43moccurrence_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_prev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransplant_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresidual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:783\u001b[0m, in \u001b[0;36mLLamaExamineToolkit.transplant_newline_activities\u001b[1;34m(self, source_strings, target_strings, num_new_tokens, target_substring, occurrence_index, num_prev, num_fut, transplant_strings)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_fut \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;66;03m# Extract newline activations from source strings\u001b[39;00m\n\u001b[0;32m    782\u001b[0m activation_containers, source_newline_indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 783\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_newline_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_substring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_substring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43moccurrence_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moccurrence_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransplant_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransplant_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_prev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_prev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_fut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_fut\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m )\n\u001b[0;32m    792\u001b[0m output_strings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# Process each target string with corresponding source activations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:552\u001b[0m, in \u001b[0;36mLLamaExamineToolkit.extract_newline_activations\u001b[1;34m(self, strings, target_substring, occurrence_index, transplant_strings, num_prev, num_fut)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracting token activations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# Compute token indices and cutoff positions for all strings\u001b[39;00m\n\u001b[0;32m    551\u001b[0m index_pairs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentify_target_token_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_substring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_substring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43moccurrence_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moccurrence_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m string \u001b[38;5;129;01min\u001b[39;00m strings\n\u001b[0;32m    558\u001b[0m ]\n\u001b[0;32m    560\u001b[0m activation_containers \u001b[38;5;241m=\u001b[39m [ActivationContainer() \u001b[38;5;28;01mfor\u001b[39;00m string \u001b[38;5;129;01min\u001b[39;00m strings]\n\u001b[0;32m    562\u001b[0m source_token_indices \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:277\u001b[0m, in \u001b[0;36mLLamaExamineToolkit.identify_target_token_index\u001b[1;34m(self, string, target_substring, occurrence_index)\u001b[0m\n\u001b[0;32m    274\u001b[0m     start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Move past this occurrence\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m occurrences \u001b[38;5;129;01mor\u001b[39;00m occurrence_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(occurrences):\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_substring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m occurrence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccurrence_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    281\u001b[0m target_start, target_end \u001b[38;5;241m=\u001b[39m occurrences[occurrence_index]\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Find the tokens that contain any part of the target string\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Target string '\n\n' occurrence 0 not found in string"
     ]
    }
   ],
   "source": [
    "# choose a model \n",
    "llama_model_string = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "out_reason = tk.transplant_newline_activities(\n",
    "    source_strings=[\"<｜User｜>Explain fractals in 150 words<｜Assistant｜>\",]*5,\n",
    "    target_strings=['<｜Assistant｜>',]*5,\n",
    "    target_substring='<｜Assistant｜>',\n",
    "    num_new_tokens=100,\n",
    "    occurrence_index=0,\n",
    "    num_prev=0,\n",
    "    num_fut=0,\n",
    "    transplant_strings=('residual','key', 'value', 'output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  <｜User｜>Word Problem Setup: A girl has ten plums when she leaves the grocery store<｜Assistant｜> \n",
      "Question: <｜Assistant｜> Therefore, the total number of plums purchased was \n",
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:50:35,516 bd95cdcf-1419-48a2-8032-fc74e9adb024 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:50:36,025 bd95cdcf-1419-48a2-8032-fc74e9adb024 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:50:36,900 bd95cdcf-1419-48a2-8032-fc74e9adb024 - RUNNING: Your job has started running.\n",
      "2025-03-15 16:50:37,764 bd95cdcf-1419-48a2-8032-fc74e9adb024 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 5.15M/5.15M [00:00<00:00, 12.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_newline_indices [18]\n",
      "source_newline_index 18\n",
      "('<｜Assistant｜>', 128012)\n",
      "{}\n",
      "generating with transplant\n",
      "we are transplanting residual\n",
      "source_token =  18 ('<｜Assistant｜>', 128012)\n",
      "these are toks [128000, 128012, 15636, 11, 279, 2860, 1396, 315, 628, 6370, 15075, 574, 220] 128012\n",
      "target_token =  1 <｜Assistant｜>\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n",
      "we are transplanting residual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:50:40,453 9cac7e92-fea0-4c1d-a714-171ef30d856e - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:50:41,149 9cac7e92-fea0-4c1d-a714-171ef30d856e - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:50:41,567 9cac7e92-fea0-4c1d-a714-171ef30d856e - RUNNING: Your job has started running.\n",
      "2025-03-15 16:50:42,243 9cac7e92-fea0-4c1d-a714-171ef30d856e - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 3.34M/3.34M [00:00<00:00, 8.39MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.11493165, 0.5836711 , 0.2147206 , 0.03616485, 0.02193509,\n",
       "       0.01372663, 0.00431928, 0.00347064, 0.00133805, 0.00572211],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a model \n",
    "llama_model_string = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "run_simple_number_experiment(tk, num=10, number_samples=1,target_substring=\"<｜Assistant｜>\", mode='reason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:50:43,847 73733c63-b7d8-488d-a57f-19277fa8daaa - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:50:44,526 73733c63-b7d8-488d-a57f-19277fa8daaa - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:50:45,235 73733c63-b7d8-488d-a57f-19277fa8daaa - RUNNING: Your job has started running.\n",
      "2025-03-15 16:50:46,419 73733c63-b7d8-488d-a57f-19277fa8daaa - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.56k/1.56k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "with llama.generate(\n",
    "        '<｜User｜>Word Problem Setup: A woman has ten oranges when he leaves the market.<｜Assistant｜> The total number of oranges was ',\n",
    "        max_new_tokens=20,\n",
    "        remote=True,\n",
    "    ) as tracer:\n",
    "    out = llama.generator.output.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:50:48,693 c666cfcf-580f-442c-9d66-5963923bcccc - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:50:49,127 c666cfcf-580f-442c-9d66-5963923bcccc - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:50:49,513 c666cfcf-580f-442c-9d66-5963923bcccc - RUNNING: Your job has started running.\n",
      "2025-03-15 16:51:04,361 c666cfcf-580f-442c-9d66-5963923bcccc - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 250M/250M [00:32<00:00, 7.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:51:39,926 92397b39-c8ef-4c1a-8095-c43657fdc2df - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:51:40,815 92397b39-c8ef-4c1a-8095-c43657fdc2df - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:51:41,788 92397b39-c8ef-4c1a-8095-c43657fdc2df - RUNNING: Your job has started running.\n",
      "2025-03-15 16:51:44,607 92397b39-c8ef-4c1a-8095-c43657fdc2df - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.63k/1.63k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:51:48,092 b7355e3f-0194-4180-aba5-675cdbe46da8 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:51:48,799 b7355e3f-0194-4180-aba5-675cdbe46da8 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:51:49,757 b7355e3f-0194-4180-aba5-675cdbe46da8 - RUNNING: Your job has started running.\n",
      "2025-03-15 16:51:51,970 b7355e3f-0194-4180-aba5-675cdbe46da8 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.56k/1.56k [00:00<00:00, 1.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:51:55,564 7c202265-c733-4057-b275-7a8721f05082 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:51:56,271 7c202265-c733-4057-b275-7a8721f05082 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:51:57,529 7c202265-c733-4057-b275-7a8721f05082 - RUNNING: Your job has started running.\n",
      "2025-03-15 16:52:02,071 7c202265-c733-4057-b275-7a8721f05082 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 2.01k/2.01k [00:00<00:00, 2.02MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:52:06,280 cfed7404-8d84-40bc-a065-bd361d5d0ef6 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:52:07,190 cfed7404-8d84-40bc-a065-bd361d5d0ef6 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:52:08,144 cfed7404-8d84-40bc-a065-bd361d5d0ef6 - RUNNING: Your job has started running.\n",
      "2025-03-15 16:52:11,144 cfed7404-8d84-40bc-a065-bd361d5d0ef6 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.50k/1.50k [00:00<00:00, 594kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n",
      "residual\n",
      "source_token =  36 ('<｜Assistant｜>', 128012)\n",
      "target_token =  1 ('<｜Assistant｜>',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 16:52:14,878 d472e331-343c-4917-997d-a6bf2fbe22dd - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 16:52:16,233 d472e331-343c-4917-997d-a6bf2fbe22dd - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-15 16:52:17,715 d472e331-343c-4917-997d-a6bf2fbe22dd - RUNNING: Your job has started running.\n",
      "2025-03-15 16:52:21,624 d472e331-343c-4917-997d-a6bf2fbe22dd - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.88k/1.88k [00:00<00:00, 1.88MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<｜begin▁of▁sentence｜><｜Assistant｜><think>\\n\\n</think>\\n\\nSure! Could you clarify or provide more details about what you'd like assistance with? Whether it's a question, a problem to solve, or something else, feel free to share, and I'll do my best to help. 😊<｜end▁of▁sentence｜>\",\n",
       " '<｜begin▁of▁sentence｜><｜Assistant｜><think>\\n\\n</think>\\n\\n**Question:**  \\nWhat is the answer to the question: \"What is the square root of 64?\" \\n\\n**Answer:**  \\nThe square root of 64 is **8**.<｜end▁of▁sentence｜>',\n",
       " '<｜begin▁of▁sentence｜><｜Assistant｜><think>\\nAlright, so I\\'ve got this riddle here: \"What has keys but can\\'t open locks?\" Hmm, at first glance, it seems a bit tricky, but maybe if I break it down, I can figure it out. Let me think about what I know about keys and locks. Keys are typically used to open locks, right? So if something has keys, you might think it\\'s related to opening things. But the question says it can\\'t open locks, which is confusing',\n",
       " \"<｜begin▁of▁sentence｜><｜Assistant｜><think>\\n\\n</think>\\n\\nSure! Let me know what you'd like to talk about or ask your question, and I'll do my best to help. 😊<｜end▁of▁sentence｜>\",\n",
       " \"<｜begin▁of▁sentence｜><｜Assistant｜><think>\\n\\n</think>\\n\\nSure! Could you clarify or provide more details about what you're asking? For example:\\n\\n- Are you looking for a specific type of information, such as a definition, explanation, or example?\\n- Are you asking about a particular topic, concept, or problem?\\n- Do you need help with something creative, like writing, math, or art?\\n\\nLet me know how I can assist! 😊<｜end▁of▁sentence｜>\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.transplant_newline_activities(\n",
    "    source_strings=[\"<｜User｜>Word Problem: A man has one apple. Then he loses one. Then he finds one more apple. If he gains two more apples, how many will he have?<｜Assistant｜>\",]*5,\n",
    "    target_strings=['<｜Assistant｜>',]*5,\n",
    "    target_substring='<｜Assistant｜>',\n",
    "    num_new_tokens=100,\n",
    "    occurrence_index=0,\n",
    "    num_prev=0,\n",
    "    num_fut=0,\n",
    "    transplant_strings=('residual',)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜User｜>Word Problem Setup: A woman has ten oranges when he leaves the market.<｜Assistant｜> The total number of oranges was 10.\\n\\nOkay, so I've got this problem where a woman has ten oranges when she leaves the\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llama.tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jhg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjhg\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jhg' is not defined"
     ]
    }
   ],
   "source": [
    "jhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Word Problem Setup: A boy has ten pears when he leaves the grocery store<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      " <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      " Therefore, the total number of pears they currently had was\n",
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:32:07,178 fb113c8b-545f-440f-9506-305235816b37 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-15 15:32:08,841 fb113c8b-545f-440f-9506-305235816b37 - APPROVED: Your job was approved and is waiting to be run.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# choose a model \n",
    "llama_model_string = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "if remote and (llama_model_string not in NDIF_models):\n",
    "    remote = False \n",
    "    print(\"Model not available on NDIF\")\n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n",
    "\n",
    "# commented out for now\n",
    "tk = LLamaExamineToolkit(\n",
    "    llama_model=llama, \n",
    "    remote=True, # use NDIF\n",
    ")\n",
    "\n",
    "run_simple_number_experiment(tk, num=10, number_samples=1,target_substring='<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n', mode='instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word Problem Setup: A woman has ten oranges when he leaves the market.\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_simple_number_string(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord Problem Setup: A woman has ten oranges when he leaves the market.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m The total number of oranges was \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\intervention\\contexts\\interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[0;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\base.py:82\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     78\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     80\u001b[0m graph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\intervention\\backends\\remote.py:77\u001b[0m, in \u001b[0;36mRemoteBackend.__call__\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocking:\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m         \u001b[38;5;66;03m# Do blocking request.\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocking_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m         \u001b[38;5;66;03m# Otherwise we are getting the status / result of the existing job.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_blocking_request(graph)\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\intervention\\backends\\remote.py:294\u001b[0m, in \u001b[0;36mRemoteBackend.blocking_request\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    291\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39msid\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Submit request via\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m LocalContext\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_send(\u001b[38;5;241m*\u001b[39margs, job_id\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mid, sio\u001b[38;5;241m=\u001b[39msio)\n\u001b[0;32m    298\u001b[0m )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# Loop until\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\intervention\\backends\\remote.py:201\u001b[0m, in \u001b[0;36mRemoteBackend.submit_request\u001b[1;34m(self, data, headers)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mreason\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(msg)\n",
      "\u001b[1;31mConnectionError\u001b[0m: Internal Server Error"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with llama.generate(\n",
    "            'Word Problem Setup: A woman has ten oranges when he leaves the market.\\n\\n The total number of oranges was ',\n",
    "            max_new_tokens=20,\n",
    "            remote=True,\n",
    "        ) as tracer:\n",
    "        out = llama.generator.output.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 17:04:25,407 003a8792-56b6-4057-9a44-6d46f24a10b7 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-14 17:04:25,927 003a8792-56b6-4057-9a44-6d46f24a10b7 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-14 17:04:27,093 003a8792-56b6-4057-9a44-6d46f24a10b7 - RUNNING: Your job has started running.\n",
      "2025-03-14 17:04:27,652 003a8792-56b6-4057-9a44-6d46f24a10b7 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.31k/1.31k [00:00<00:00, 48.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "n_new_tokens = 3\n",
    "with llama.generate(prompt, max_new_tokens=n_new_tokens, remote=True) as tracer:\n",
    "    out = llama.generator.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Word Problem Setup: A woman has ten oranges when he leaves the market.\\n\\n The total number of oranges was 10. A woman has ten oranges when he leaves the market. He gives three to his daughter.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 17:24:11,486 7a1970e4-1246-42eb-a17f-bbc072cfc38b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-14 17:24:11,930 7a1970e4-1246-42eb-a17f-bbc072cfc38b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-14 17:24:12,520 7a1970e4-1246-42eb-a17f-bbc072cfc38b - RUNNING: Your job has started running.\n",
      "2025-03-14 17:24:14,806 7a1970e4-1246-42eb-a17f-bbc072cfc38b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 21.3M/21.3M [00:02<00:00, 10.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating with transplant\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs\u001b[38;5;241m=\u001b[39m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransplant_newline_activities\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord Problem Setup: A woman has ten oranges when he leaves the market.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m She has \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord Problem Setup: A woman has ten oranges when he leaves the market.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m She has \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_substring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moccurrence_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_prev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_fut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransplant_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresidual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:792\u001b[0m, in \u001b[0;36mtransplant_newline_activities\u001b[1;34m(self, source_strings, target_strings, num_new_tokens, target_substring, occurrence_index, num_prev, num_fut, transplant_strings)\u001b[0m\n\u001b[0;32m    787\u001b[0m output_strings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Process each target string with corresponding source activations\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_string, activation_container, source_newline_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    791\u001b[0m     target_strings, activation_containers, source_newline_indices\n\u001b[1;32m--> 792\u001b[0m ):\n\u001b[0;32m    793\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_with_transplanted_activity(\n\u001b[0;32m    794\u001b[0m         target_string\u001b[38;5;241m=\u001b[39mtarget_string,\n\u001b[0;32m    795\u001b[0m         target_substring\u001b[38;5;241m=\u001b[39mtarget_substring,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    802\u001b[0m         num_fut\u001b[38;5;241m=\u001b[39mnum_fut,\n\u001b[0;32m    803\u001b[0m     )\n\u001b[0;32m    804\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:672\u001b[0m, in \u001b[0;36mgenerate_with_transplanted_activity\u001b[1;34m(self, target_string, target_substring, source_token_index, activation_container, num_new_tokens, occurrence_index, num_prev, num_fut, transplant_strings)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\intervention\\contexts\\interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[0;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\durrc\\OneDrive\\Desktop\\mech_interp_work\\mech_interp_research\\activation_transplanting.py:722\u001b[0m, in \u001b[0;36mgenerate_with_transplanted_activity\u001b[1;34m(self, target_string, target_substring, source_token_index, activation_container, num_new_tokens, occurrence_index, num_prev, num_fut, transplant_strings)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama_config\u001b[38;5;241m.\u001b[39mnum_hidden_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39moutput[\n\u001b[0;32m    711\u001b[0m             :, target_token_idx \u001b[38;5;241m+\u001b[39m delta, :\n\u001b[0;32m    712\u001b[0m         ] \u001b[38;5;241m=\u001b[39m activation_container\u001b[38;5;241m.\u001b[39mget_activation(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m             label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_residual_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    716\u001b[0m         )\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_token = \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    720\u001b[0m     target_token_idx \u001b[38;5;241m+\u001b[39m delta,\n\u001b[0;32m    721\u001b[0m     activation_container\u001b[38;5;241m.\u001b[39mget_token_by_index(\n\u001b[1;32m--> 722\u001b[0m         target_token_idx \u001b[38;5;241m+\u001b[39m delta\n\u001b[0;32m    723\u001b[0m     )\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    724\u001b[0m )\n\u001b[0;32m    725\u001b[0m toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(cutoff_string)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_token = \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    728\u001b[0m     target_token_idx \u001b[38;5;241m+\u001b[39m delta,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m     ),\n\u001b[0;32m    734\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "outputs=tk.transplant_newline_activities(\n",
    "        source_strings=['Word Problem Setup: A woman has ten oranges when he leaves the market.\\n\\n She has '],\n",
    "        target_strings=['Word Problem Setup: A woman has ten oranges when he leaves the market.\\n\\n She has '],\n",
    "        num_new_tokens=10,\n",
    "        target_substring=\".\\n\\n\",\n",
    "        occurrence_index= 0,\n",
    "        num_prev = 0,\n",
    "        num_fut= 0,\n",
    "        transplant_strings= (\"residual\",),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting token activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:54:10,082 eaa305d2-3b3e-49cc-8288-593753fe095c - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-14 16:54:11,607 eaa305d2-3b3e-49cc-8288-593753fe095c - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-14 16:54:13,479 eaa305d2-3b3e-49cc-8288-593753fe095c - RUNNING: Your job has started running.\n",
      "2025-03-14 16:54:19,099 eaa305d2-3b3e-49cc-8288-593753fe095c - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 4.62M/4.62M [00:00<00:00, 16.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_newline_indices [16]\n",
      "source_newline_index 16\n",
      "('.\\n\\n', 382)\n",
      "{}\n",
      "generating with transplant\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n",
      "source_token =  11 (' when', 994)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -4 .\n",
      "\n",
      "\n",
      "source_token =  12 (' he', 568)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -3 .\n",
      "\n",
      "\n",
      "source_token =  13 (' leaves', 11141)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -2 They\n",
      "source_token =  14 (' the', 279)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  -1  have\n",
      "source_token =  15 (' store', 3637)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  0 <|begin_of_text|>\n",
      "source_token =  16 ('.\\n\\n', 382)\n",
      "these are toks [128000, 11116, 22854, 19139, 25, 362, 893, 706, 5899, 1069, 14576, 994, 568, 11141, 279, 3637, 382, 382, 7009, 617] 11116\n",
      "target_token =  1 Word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 16:54:38,784 4f43751e-117a-4c53-ad48-b664cd88ab7b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-14 16:54:43,241 4f43751e-117a-4c53-ad48-b664cd88ab7b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-14 16:54:49,764 4f43751e-117a-4c53-ad48-b664cd88ab7b - RUNNING: Your job has started running.\n",
      "2025-03-14 16:54:57,709 4f43751e-117a-4c53-ad48-b664cd88ab7b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 5.13M/5.13M [00:00<00:00, 7.89MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.11248883, 0.1583274 , 0.04535482, 0.08479982, 0.13980855,\n",
       "       0.03025266, 0.04424512, 0.04272186, 0.01677323, 0.32522765],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_number_experiment(tk, num=10, number_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll see if we can read from this how many fruits there were "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets generate a bunch of random strings for each number in 1-10 \n",
    "\n",
    "we'll try to see if we can extract from this the placement of the vectors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
