{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2ac765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sda1048/Desktop/interp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from nnsight import CONFIG\n",
    "from nnsight import LanguageModel\n",
    "import nnsight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import yaml \n",
    "import glob\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import pickle\n",
    "# importing from my own code \n",
    "from activation_transplanting import *\n",
    "from analysis_tools import *\n",
    "\n",
    "# Save object to a file\n",
    "def save_pickle(obj, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Load object from a file\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9532fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a model \n",
    "llama_model_string = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# remote = use NDIF\n",
    "remote = True \n",
    "\n",
    "# load a model\n",
    "llama = LanguageModel(llama_model_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975fce68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 128014], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer(\"</think>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f17694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(ac):\n",
    "    out = [None]*len(ac.token_idx_to_value_dict)\n",
    "    tokens = [None]*len(ac.token_idx_to_value_dict)\n",
    "    for i in ac.token_idx_to_value_dict:\n",
    "        out[i] = ac.token_idx_to_value_dict[i][0]\n",
    "        tokens[i] = ac.token_idx_to_value_dict[i][1]\n",
    "\n",
    "    return ''.join(out), tokens\n",
    "\n",
    "def extract_first_think_answer_tokens(ac, collection_length = 20):\n",
    "    string, tokens = extract_output(ac) \n",
    "    think_start = string.index(\"<think>\\n\") + len(\"<think>\\n\")\n",
    "    think_id_start = tokens.index(128013) + 1\n",
    "\n",
    "    if \"</think>\" in string:\n",
    "        answer_start = string.index(\"</think>\") + len(\"</think>\")\n",
    "        answer_id_start = tokens.index(128014) + 1\n",
    "\n",
    "        return (\n",
    "            string[think_start:think_start+collection_length], \n",
    "            tokens[think_id_start:think_id_start+collection_length]\n",
    "        ), (\n",
    "            string[answer_start:answer_start+collection_length],\n",
    "            tokens[answer_id_start:answer_id_start+collection_length]\n",
    "        )\n",
    "\n",
    "    return (\n",
    "            string[think_start:think_start+collection_length], \n",
    "            tokens[think_id_start:think_id_start+collection_length]\n",
    "        ), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea5e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "think_start_strings = []\n",
    "answer_start_strings = []\n",
    "\n",
    "for ac_path in glob.glob(\"activity_container_pickles/*.pkl\"):\n",
    "    ac = load_pickle(ac_path)\n",
    "    \n",
    "    think, answer = extract_first_think_answer_tokens(ac)\n",
    "\n",
    "    think_start_strings.append(think)\n",
    "\n",
    "    if answer is not None:\n",
    "        answer_start_strings.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6ae42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7fa2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I'm trying \n",
      "To find the area of \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "First, I need to cal\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I have this\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "To find the result o\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To determine the fra\n",
      "Okay, so I'm trying \n",
      "First, I will list a\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I have this\n",
      "Okay, so I need to f\n",
      "The given sequence i\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "I start with the equ\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "I need to determine \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "First, I recognize t\n",
      "First, I recognize t\n",
      "I need to find the p\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "First, I need to eva\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To determine the sum\n",
      "First, I recognize t\n",
      "First, I need to sol\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "I need to determine \n",
      "Okay, so I need to f\n",
      "First, I recognize t\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I have this\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To solve the express\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To find the value of\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "First, I need to fin\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I have this\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "First, I need to cal\n",
      "Okay, so I need to f\n",
      "First, I recognize t\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "To determine the squ\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "First, I need to det\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "First, I need to ide\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "To convert 0.35 into\n",
      "Okay, so I need to f\n",
      "To find the sum of t\n",
      "First, I recognize t\n",
      "Okay, so I'm trying \n",
      "To find the prime fa\n",
      "First, I'll define t\n",
      "Okay, so I'm trying \n",
      "Alright, so I need t\n",
      "To find 40% of 85, I\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "First, I need to sol\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "I need to calculate \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To find the length o\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "I need to find 15% o\n",
      "Okay, so I need to f\n",
      "First, I need to fol\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "To solve (2 + 7) × 4\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "I need to find the o\n",
      "Okay, so I need to f\n",
      "Okay, so I have this\n",
      "Okay, so I need to e\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "First, I need to det\n",
      "Okay, so I'm trying \n",
      "First, I need to det\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "To find the area of \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To find the least co\n",
      "First, I observe the\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To find the median o\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To determine what 2/\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "First, I need to sol\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "First, I need to sol\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I have this\n",
      "First, I need to ide\n",
      "Alright, so I need t\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I have this\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "To multiply 1.5 by 0\n",
      "Okay, so I need to f\n",
      "First, I need to fin\n",
      "Okay, so I need to f\n",
      "To find the integer \n",
      "Okay, so I need to f\n",
      "Okay, so I need to i\n",
      "Okay, so I'm trying \n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "Okay, so I'm trying \n",
      "Okay, so I need to f\n",
      "To solve 7 squared m\n",
      "Okay, so I need to f\n"
     ]
    }
   ],
   "source": [
    "think_1, think2 = zip(*think_start_strings) \n",
    "for s in think_1:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57499da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 33413, 11, 779, 358, 1205, 311, 282], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer(\"Okay, so I need to f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a002157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜>Okay'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer.decode([128000, 33413,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe341ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To find the area o\n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "To find the result\n",
      "\n",
      "\n",
      "\"Themselves\" is a \n",
      "\n",
      "\n",
      "To determine the f\n",
      "\n",
      "\n",
      "To find the **mode\n",
      "\n",
      "\n",
      "To determine the n\n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "To find the **peri\n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the sum of\n",
      "\n",
      "\n",
      "To find the **half\n",
      "\n",
      "\n",
      "To solve the equat\n",
      "\n",
      "\n",
      "To determine how m\n",
      "\n",
      "\n",
      "To determine the p\n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We \n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We \n",
      "\n",
      "\n",
      "To find the square\n",
      "\n",
      "\n",
      "The tissue that co\n",
      "\n",
      "\n",
      "To determine how m\n",
      "\n",
      "\n",
      "To determine the s\n",
      "\n",
      "\n",
      "To convert **0.35*\n",
      "\n",
      "\n",
      "To find the **sum \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the **prim\n",
      "\n",
      "\n",
      "Let's solve the pr\n",
      "\n",
      "\n",
      "To find **40%** of\n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "To find the volume\n",
      "\n",
      "\n",
      "To find the **leng\n",
      "\n",
      "\n",
      "To find **15% of 8\n",
      "\n",
      "\n",
      "Let's solve the ex\n",
      "\n",
      "\n",
      "The subject of the\n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the origin\n",
      "\n",
      "\n",
      "The literary devic\n",
      "\n",
      "\n",
      "To determine the c\n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the area o\n",
      "\n",
      "\n",
      "To find the **Leas\n",
      "\n",
      "\n",
      "To determine the n\n",
      "\n",
      "\n",
      "To find the **medi\n",
      "\n",
      "\n",
      "To find **\\(\\frac{\n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "To find the value \n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To \n",
      "\n",
      "\n",
      "To find the produc\n",
      "\n",
      "\n",
      "To find the **aver\n",
      "\n",
      "\n",
      "To determine which\n",
      "\n",
      "\n",
      "The verb in the se\n",
      "\n",
      "\n",
      "The sentence \"If i\n",
      "\n",
      "\n",
      "To solve \\(7^2 - 3\n"
     ]
    }
   ],
   "source": [
    "ans1, ans2 = zip(*answer_start_strings) \n",
    "\n",
    "for a1 in ans1:\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916afe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer.decode([271])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d11fb10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3158,\n",
       "  315,\n",
       "  264,\n",
       "  9518,\n",
       "  994,\n",
       "  1202,\n",
       "  47442,\n",
       "  374,\n",
       "  2728,\n",
       "  11,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  19,\n",
       "  61,\n",
       "  17,\n",
       "  482,\n",
       "  220,\n",
       "  18,\n",
       "  61,\n",
       "  18,\n",
       "  59,\n",
       "  705,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  1121,\n",
       "  315,\n",
       "  50802,\n",
       "  3146,\n",
       "  8929,\n",
       "  334,\n",
       "  555,\n",
       "  3146,\n",
       "  717,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  4382,\n",
       "  7504,\n",
       "  1473,\n",
       "  16],\n",
       " [271,\n",
       "  1,\n",
       "  1016,\n",
       "  12116,\n",
       "  4372,\n",
       "  1,\n",
       "  374,\n",
       "  264,\n",
       "  33766,\n",
       "  535,\n",
       "  19126,\n",
       "  1656,\n",
       "  1511,\n",
       "  304,\n",
       "  279,\n",
       "  39598,\n",
       "  1376,\n",
       "  13,\n",
       "  94493,\n",
       "  535],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  19983,\n",
       "  315,\n",
       "  279,\n",
       "  23317,\n",
       "  430,\n",
       "  8625,\n",
       "  1306,\n",
       "  12459,\n",
       "  220,\n",
       "  18,\n",
       "  35354,\n",
       "  704,\n",
       "  315,\n",
       "  220,\n",
       "  23,\n",
       "  11],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  8684,\n",
       "  334,\n",
       "  315,\n",
       "  279,\n",
       "  5219,\n",
       "  18240,\n",
       "  20,\n",
       "  11,\n",
       "  220,\n",
       "  23,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  11,\n",
       "  220],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  1828,\n",
       "  1396,\n",
       "  304,\n",
       "  279,\n",
       "  8668,\n",
       "  18240,\n",
       "  220,\n",
       "  18,\n",
       "  11,\n",
       "  220,\n",
       "  22,\n",
       "  11,\n",
       "  220,\n",
       "  806,\n",
       "  11,\n",
       "  220],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  379,\n",
       "  1144,\n",
       "  8,\n",
       "  994,\n",
       "  18240,\n",
       "  379,\n",
       "  284,\n",
       "  220,\n",
       "  18,\n",
       "  87,\n",
       "  489,\n",
       "  220,\n",
       "  17],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  1505,\n",
       "  704,\n",
       "  1268,\n",
       "  1790,\n",
       "  220,\n",
       "  23,\n",
       "  41776,\n",
       "  2853,\n",
       "  422,\n",
       "  220,\n",
       "  20,\n",
       "  41776,\n",
       "  2853,\n",
       "  33982],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  1144,\n",
       "  38118,\n",
       "  90,\n",
       "  19,\n",
       "  15523,\n",
       "  20,\n",
       "  92,\n",
       "  1144,\n",
       "  614,\n",
       "  1144,\n",
       "  38118,\n",
       "  90,\n",
       "  17],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  17,\n",
       "  61,\n",
       "  18,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  61,\n",
       "  17,\n",
       "  59,\n",
       "  705,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  716,\n",
       "  26402,\n",
       "  334,\n",
       "  315,\n",
       "  264,\n",
       "  23596,\n",
       "  11,\n",
       "  499,\n",
       "  649,\n",
       "  1005,\n",
       "  279,\n",
       "  15150,\n",
       "  1473,\n",
       "  59,\n",
       "  9837],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  1114,\n",
       "  1144,\n",
       "  15487,\n",
       "  220,\n",
       "  21,\n",
       "  482,\n",
       "  220,\n",
       "  19,\n",
       "  61],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  2694,\n",
       "  315,\n",
       "  279,\n",
       "  19016,\n",
       "  304,\n",
       "  279,\n",
       "  1396,\n",
       "  3146,\n",
       "  8878,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  38106,\n",
       "  3195,\n",
       "  1486,\n",
       "  334,\n",
       "  1990,\n",
       "  279,\n",
       "  43318,\n",
       "  323,\n",
       "  50937,\n",
       "  3585,\n",
       "  315,\n",
       "  3090,\n",
       "  11,\n",
       "  1833,\n",
       "  1521],\n",
       " [271,\n",
       "  1271,\n",
       "  11886,\n",
       "  279,\n",
       "  24524,\n",
       "  18240,\n",
       "  17,\n",
       "  87,\n",
       "  489,\n",
       "  220,\n",
       "  22,\n",
       "  284,\n",
       "  220,\n",
       "  18,\n",
       "  87,\n",
       "  482,\n",
       "  220,\n",
       "  20,\n",
       "  59,\n",
       "  705],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  1268,\n",
       "  1690,\n",
       "  2919,\n",
       "  433,\n",
       "  690,\n",
       "  1935,\n",
       "  311,\n",
       "  6381,\n",
       "  5403,\n",
       "  264,\n",
       "  2363,\n",
       "  449,\n",
       "  3146,\n",
       "  6843,\n",
       "  6959,\n",
       "  334,\n",
       "  422],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  19463,\n",
       "  315,\n",
       "  3794,\n",
       "  7041,\n",
       "  220,\n",
       "  17,\n",
       "  14971,\n",
       "  994,\n",
       "  65761,\n",
       "  264,\n",
       "  6762,\n",
       "  16652,\n",
       "  220,\n",
       "  18,\n",
       "  3115,\n",
       "  11],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1687,\n",
       "  527,\n",
       "  311,\n",
       "  15806,\n",
       "  279,\n",
       "  7645,\n",
       "  512,\n",
       "  59,\n",
       "  9837,\n",
       "  7,\n",
       "  22,\n",
       "  489,\n",
       "  220,\n",
       "  18,\n",
       "  8],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1687,\n",
       "  527,\n",
       "  2728,\n",
       "  279,\n",
       "  24524,\n",
       "  512,\n",
       "  79145,\n",
       "  220,\n",
       "  17,\n",
       "  87,\n",
       "  489,\n",
       "  220,\n",
       "  18,\n",
       "  88,\n",
       "  284],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  1144,\n",
       "  38118,\n",
       "  90,\n",
       "  20,\n",
       "  15523,\n",
       "  23,\n",
       "  92,\n",
       "  489,\n",
       "  1144,\n",
       "  38118,\n",
       "  90,\n",
       "  18,\n",
       "  15523],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  8417,\n",
       "  1268,\n",
       "  3117,\n",
       "  279,\n",
       "  1841,\n",
       "  690,\n",
       "  5944,\n",
       "  304,\n",
       "  220,\n",
       "  17,\n",
       "  13,\n",
       "  20,\n",
       "  4207,\n",
       "  520],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1687,\n",
       "  527,\n",
       "  2728,\n",
       "  279,\n",
       "  11595,\n",
       "  315,\n",
       "  13305,\n",
       "  311,\n",
       "  7724,\n",
       "  304,\n",
       "  264,\n",
       "  538,\n",
       "  439,\n",
       "  18240,\n",
       "  220],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  9518,\n",
       "  3789,\n",
       "  315,\n",
       "  3146,\n",
       "  8929,\n",
       "  98319,\n",
       "  584,\n",
       "  649,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146,\n",
       "  16648],\n",
       " [271,\n",
       "  791,\n",
       "  20438,\n",
       "  430,\n",
       "  34161,\n",
       "  16124,\n",
       "  311,\n",
       "  17685,\n",
       "  374,\n",
       "  279,\n",
       "  88932,\n",
       "  13,\n",
       "  350,\n",
       "  408,\n",
       "  2439,\n",
       "  527,\n",
       "  961,\n",
       "  315,\n",
       "  279,\n",
       "  4667],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  1268,\n",
       "  1690,\n",
       "  2919,\n",
       "  433,\n",
       "  1053,\n",
       "  1935,\n",
       "  369,\n",
       "  220,\n",
       "  20,\n",
       "  1274,\n",
       "  311,\n",
       "  6308,\n",
       "  279,\n",
       "  3838,\n",
       "  11,\n",
       "  584,\n",
       "  649],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  4732,\n",
       "  315,\n",
       "  279,\n",
       "  5542,\n",
       "  11,\n",
       "  584,\n",
       "  649,\n",
       "  1005,\n",
       "  279,\n",
       "  6913,\n",
       "  15150,\n",
       "  369,\n",
       "  4732,\n",
       "  1473,\n",
       "  59,\n",
       "  9837],\n",
       " [271,\n",
       "  1271,\n",
       "  5625,\n",
       "  3146,\n",
       "  15,\n",
       "  13,\n",
       "  1758,\n",
       "  334,\n",
       "  1139,\n",
       "  264,\n",
       "  19983,\n",
       "  323,\n",
       "  40821,\n",
       "  433,\n",
       "  11,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  1264,\n",
       "  315,\n",
       "  279,\n",
       "  1176,\n",
       "  220,\n",
       "  605,\n",
       "  26864,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  8417,\n",
       "  1268,\n",
       "  1690,\n",
       "  2919,\n",
       "  433,\n",
       "  1053,\n",
       "  1935,\n",
       "  369,\n",
       "  220,\n",
       "  23,\n",
       "  7487,\n",
       "  311,\n",
       "  1977,\n",
       "  279],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  33438,\n",
       "  8331,\n",
       "  2065,\n",
       "  334,\n",
       "  315,\n",
       "  3146,\n",
       "  3487,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146],\n",
       " [271,\n",
       "  10267,\n",
       "  596,\n",
       "  11886,\n",
       "  279,\n",
       "  3575,\n",
       "  3094,\n",
       "  555,\n",
       "  3094,\n",
       "  382,\n",
       "  334,\n",
       "  22818,\n",
       "  25,\n",
       "  1035,\n",
       "  12,\n",
       "  578,\n",
       "  6811,\n",
       "  1990,\n",
       "  1403,\n",
       "  5219],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  3146,\n",
       "  1272,\n",
       "  4,\n",
       "  334,\n",
       "  315,\n",
       "  3146,\n",
       "  5313,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146,\n",
       "  12281,\n",
       "  279],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  865,\n",
       "  1144,\n",
       "  8,\n",
       "  304,\n",
       "  279,\n",
       "  24524,\n",
       "  18240,\n",
       "  220,\n",
       "  18,\n",
       "  87,\n",
       "  482,\n",
       "  220,\n",
       "  22],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  8286,\n",
       "  315,\n",
       "  264,\n",
       "  24671,\n",
       "  994,\n",
       "  1855,\n",
       "  3185,\n",
       "  374,\n",
       "  220,\n",
       "  21,\n",
       "  10166,\n",
       "  1317,\n",
       "  11,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  4222,\n",
       "  334,\n",
       "  315,\n",
       "  279,\n",
       "  23596,\n",
       "  11,\n",
       "  584,\n",
       "  649,\n",
       "  1005,\n",
       "  279,\n",
       "  15150,\n",
       "  369,\n",
       "  279,\n",
       "  3158,\n",
       "  315],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  3146,\n",
       "  868,\n",
       "  4,\n",
       "  315,\n",
       "  220,\n",
       "  1490,\n",
       "  98319,\n",
       "  1833,\n",
       "  1521,\n",
       "  4228,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146,\n",
       "  16648,\n",
       "  2752],\n",
       " [271,\n",
       "  10267,\n",
       "  596,\n",
       "  11886,\n",
       "  279,\n",
       "  7645,\n",
       "  3094,\n",
       "  555,\n",
       "  3094,\n",
       "  2768,\n",
       "  279,\n",
       "  3146,\n",
       "  1382,\n",
       "  315,\n",
       "  7677,\n",
       "  334,\n",
       "  320,\n",
       "  1777,\n",
       "  6204,\n",
       "  1950],\n",
       " [271,\n",
       "  791,\n",
       "  3917,\n",
       "  315,\n",
       "  279,\n",
       "  11914,\n",
       "  330,\n",
       "  791,\n",
       "  8415,\n",
       "  7731,\n",
       "  389,\n",
       "  279,\n",
       "  5634,\n",
       "  1,\n",
       "  374,\n",
       "  3146,\n",
       "  1,\n",
       "  1820,\n",
       "  8415,\n",
       "  1210],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  11886,\n",
       "  279,\n",
       "  7645,\n",
       "  1144,\n",
       "  1209,\n",
       "  17,\n",
       "  489,\n",
       "  220,\n",
       "  22,\n",
       "  8,\n",
       "  1144,\n",
       "  15487,\n",
       "  220,\n",
       "  19],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  4113,\n",
       "  1396,\n",
       "  994,\n",
       "  220,\n",
       "  508,\n",
       "  4,\n",
       "  315,\n",
       "  433,\n",
       "  374,\n",
       "  220,\n",
       "  1758,\n",
       "  11,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473],\n",
       " [271,\n",
       "  791,\n",
       "  32465,\n",
       "  3756,\n",
       "  1511,\n",
       "  304,\n",
       "  330,\n",
       "  791,\n",
       "  23529,\n",
       "  389,\n",
       "  279,\n",
       "  5951,\n",
       "  733,\n",
       "  4883,\n",
       "  323,\n",
       "  4883,\n",
       "  1,\n",
       "  374,\n",
       "  3146,\n",
       "  9164],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  2853,\n",
       "  315,\n",
       "  220,\n",
       "  22,\n",
       "  23423,\n",
       "  11,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13,\n",
       "  3146,\n",
       "  10086,\n",
       "  279,\n",
       "  2853],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  8417,\n",
       "  1268,\n",
       "  1690,\n",
       "  3678,\n",
       "  39863,\n",
       "  499,\n",
       "  617,\n",
       "  2163,\n",
       "  1306,\n",
       "  7231,\n",
       "  3201,\n",
       "  18240,\n",
       "  1144,\n",
       "  38118],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3158,\n",
       "  315,\n",
       "  264,\n",
       "  12960,\n",
       "  994,\n",
       "  499,\n",
       "  1440,\n",
       "  1202,\n",
       "  10801,\n",
       "  11,\n",
       "  499,\n",
       "  649,\n",
       "  1005,\n",
       "  279,\n",
       "  15150,\n",
       "  1473],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  82916,\n",
       "  7874,\n",
       "  29911,\n",
       "  320,\n",
       "  8724,\n",
       "  44,\n",
       "  33395,\n",
       "  315,\n",
       "  3146,\n",
       "  21,\n",
       "  334,\n",
       "  323,\n",
       "  3146,\n",
       "  23,\n",
       "  98319],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  279,\n",
       "  1828,\n",
       "  1396,\n",
       "  304,\n",
       "  279,\n",
       "  8668,\n",
       "  18240,\n",
       "  17,\n",
       "  11,\n",
       "  220,\n",
       "  19,\n",
       "  11,\n",
       "  220,\n",
       "  23,\n",
       "  11,\n",
       "  220,\n",
       "  845],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  56751,\n",
       "  334,\n",
       "  315,\n",
       "  279,\n",
       "  5219,\n",
       "  18240,\n",
       "  717,\n",
       "  11,\n",
       "  220,\n",
       "  22,\n",
       "  11,\n",
       "  220,\n",
       "  868,\n",
       "  11,\n",
       "  220],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  3146,\n",
       "  59,\n",
       "  11781,\n",
       "  38118,\n",
       "  90,\n",
       "  17,\n",
       "  15523,\n",
       "  18,\n",
       "  11281,\n",
       "  8,\n",
       "  315,\n",
       "  1144,\n",
       "  11781,\n",
       "  38118,\n",
       "  90,\n",
       "  16,\n",
       "  15523],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  865,\n",
       "  1144,\n",
       "  8,\n",
       "  304,\n",
       "  279,\n",
       "  24524,\n",
       "  18240,\n",
       "  865,\n",
       "  489,\n",
       "  220,\n",
       "  20,\n",
       "  284,\n",
       "  220],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  907,\n",
       "  315,\n",
       "  18240,\n",
       "  220,\n",
       "  20,\n",
       "  1144,\n",
       "  15487,\n",
       "  320,\n",
       "  18,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  8,\n",
       "  482,\n",
       "  220,\n",
       "  17],\n",
       " [271,\n",
       "  334,\n",
       "  37942,\n",
       "  25,\n",
       "  57277,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  6412,\n",
       "  3430,\n",
       "  315,\n",
       "  279,\n",
       "  15845,\n",
       "  1306,\n",
       "  264,\n",
       "  220,\n",
       "  914,\n",
       "  4,\n",
       "  11336,\n",
       "  11],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  2027,\n",
       "  315,\n",
       "  18240,\n",
       "  16,\n",
       "  13,\n",
       "  20,\n",
       "  58858,\n",
       "  323,\n",
       "  18240,\n",
       "  15,\n",
       "  13,\n",
       "  23,\n",
       "  59,\n",
       "  705,\n",
       "  1833,\n",
       "  1521],\n",
       " [271,\n",
       "  1271,\n",
       "  1505,\n",
       "  279,\n",
       "  3146,\n",
       "  17645,\n",
       "  334,\n",
       "  315,\n",
       "  279,\n",
       "  5219,\n",
       "  220,\n",
       "  1114,\n",
       "  11,\n",
       "  220,\n",
       "  1419,\n",
       "  11,\n",
       "  323,\n",
       "  220,\n",
       "  966,\n",
       "  11],\n",
       " [271,\n",
       "  1271,\n",
       "  8417,\n",
       "  902,\n",
       "  7698,\n",
       "  596,\n",
       "  9518,\n",
       "  3789,\n",
       "  374,\n",
       "  18585,\n",
       "  311,\n",
       "  3146,\n",
       "  21,\n",
       "  13,\n",
       "  18,\n",
       "  98319,\n",
       "  1095,\n",
       "  596,\n",
       "  9616,\n",
       "  279],\n",
       " [271,\n",
       "  791,\n",
       "  19120,\n",
       "  304,\n",
       "  279,\n",
       "  11914,\n",
       "  330,\n",
       "  791,\n",
       "  5679,\n",
       "  293,\n",
       "  43161,\n",
       "  54945,\n",
       "  1210,\n",
       "  374,\n",
       "  3146,\n",
       "  65,\n",
       "  43161,\n",
       "  334,\n",
       "  13],\n",
       " [271,\n",
       "  791,\n",
       "  11914,\n",
       "  330,\n",
       "  2746,\n",
       "  433,\n",
       "  62555,\n",
       "  11,\n",
       "  358,\n",
       "  690,\n",
       "  4822,\n",
       "  2162,\n",
       "  1,\n",
       "  374,\n",
       "  264],\n",
       " [271,\n",
       "  1271,\n",
       "  11886,\n",
       "  18240,\n",
       "  22,\n",
       "  61,\n",
       "  17,\n",
       "  482,\n",
       "  220,\n",
       "  18,\n",
       "  61,\n",
       "  17,\n",
       "  59,\n",
       "  705,\n",
       "  1833,\n",
       "  1521,\n",
       "  7504,\n",
       "  1473,\n",
       "  16,\n",
       "  13])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6cbb00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 198, 334, 37942, 25, 1035], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer('\\n**Solution:**\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ba596ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**Solution:**\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer.decode([198, 334, 37942, 25, 1035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02e52e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 334, 37942, 25], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer(\"**Solution:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd94e7",
   "metadata": {},
   "source": [
    "Now let's compute the coefficients for computing a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65422502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "927fc7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find [128000, 1271, 1505]\n",
      "To determine [128000, 1271, 8417]\n",
      "To  [128000, 1271, 220]\n",
      "**Solution: [128000, 334, 37942, 25]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for s in [\"To find\", \"To determine\", \"To \", \"**Solution:\"]:\n",
    "    print(s, llama.tokenizer.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc45e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay\\nTo\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.tokenizer.decode([33413, 198, 1271, 271])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeea8a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay\n",
      "To\n",
      "To find\n",
      "To determine\n",
      "**Solution:\n"
     ]
    }
   ],
   "source": [
    "feature_tokens = [\n",
    "    [33413],\n",
    "    [1271],\n",
    "    [1271, 1505],\n",
    "    [1271, 8417],\n",
    "    [334, 37942, 25]\n",
    "]\n",
    "for f in feature_tokens:\n",
    "    print(llama.tokenizer.decode(f))\n",
    "\n",
    "ans_data = []\n",
    "think_data = []\n",
    "\n",
    "for a1 in ans1:\n",
    "    a1 = list(llama.tokenizer.encode(a1)[1:])\n",
    "    while a1[0] == 271:\n",
    "        a1.pop(0)\n",
    "\n",
    "    tmp = []\n",
    "    for f in feature_tokens:\n",
    "        if f == a1[:len(f)]:\n",
    "            tmp.append(1.)\n",
    "        else:\n",
    "            tmp.append(0.)\n",
    "    \n",
    "    ans_data.append(tmp)\n",
    "    \n",
    "for t2 in think2:\n",
    "    while t2[0] == 198:\n",
    "        t2.pop(0)\n",
    "    tmp=[]\n",
    "    for f in feature_tokens:\n",
    "        if f == t2[:len(f)]:\n",
    "            tmp.append(1.)\n",
    "        else:\n",
    "            tmp.append(0.)\n",
    "    \n",
    "    think_data.append(tmp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db02ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data = np.array(think_data+ans_data)\n",
    "tot_labels = np.array([0]*len(think_data) + [1]*len(ans_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaeb2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca7bdaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "    X=tot_data,\n",
    "    y=tot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1536afaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.85169441,  1.14226386,  1.00315061,  0.74422492,  2.45895541]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43a45582",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle((classifier.coef_, feature_tokens), \"answering_classifier_coefs_terms.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565dde3",
   "metadata": {},
   "source": [
    "Great! \n",
    "\n",
    "now we have some weights to tell us how to put together a metric \n",
    "\n",
    "lets import transformerlens to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dffea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "import hashlib\n",
    "import yaml \n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d14850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "\n",
    "reference_model_path = 'meta-llama/Llama-3.1-8B'\n",
    "baseline_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "baseline_model_hf = AutoModelForCausalLM.from_pretrained(baseline_model_path, torch_dtype=torch.bfloat16)\n",
    "baseline_model_tokenizer = AutoTokenizer.from_pretrained(baseline_model_path)\n",
    "\n",
    "baseline_model = HookedTransformer.from_pretrained_no_processing(\n",
    "    reference_model_path,\n",
    "    hf_model=baseline_model_hf,\n",
    "    tokenizer=baseline_model_tokenizer,\n",
    "    device=device,\n",
    "    move_to_device=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceef2997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a947bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa08ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_feature_mode_metric(\n",
    "    model: HookedTransformer,\n",
    "    prompt: str,\n",
    "    pos_features: list[list[int]],\n",
    "    neg_features: list[list[int]],\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - normalized probabilities for each feature sequence (sum to 1)\n",
    "      - unnormalized log-scores for each\n",
    "      - logit (log-odds) of producing any negative feature.\n",
    "    Uses model.to_tokens() for tokenization.\n",
    "    \"\"\"\n",
    "    device = model.cfg.device\n",
    "    # 1) Tokenize prompt with the built-in hook\n",
    "    #    (adds BOS if the model is configured to)\n",
    "    input_ids = model.to_tokens(prompt).to(device)\n",
    "\n",
    "    # 2) Score one feature sequence by accumulating log-probs\n",
    "    def sequence_score(feature: list[int]) -> torch.Tensor:\n",
    "        ctx = input_ids.clone()\n",
    "        total_log_prob = torch.tensor(0.0, device=device)\n",
    "        for tok in feature:\n",
    "            # run the model on the current context\n",
    "            logits, cache = model.run_with_cache(ctx)\n",
    "            last_logits = logits[:, -1, :]  # [1, vocab_size]\n",
    "            log_probs = torch.log_softmax(last_logits, dim=-1)\n",
    "            total_log_prob = total_log_prob + log_probs[0, tok]\n",
    "            # append the ground-truth token to the context\n",
    "            ctx = torch.cat([ctx, torch.tensor([[tok]], device=device)], dim=1)\n",
    "        return total_log_prob\n",
    "\n",
    "    # 3) Compute scores for all features\n",
    "    all_features = pos_features + neg_features\n",
    "    scores = torch.stack([sequence_score(f) for f in all_features])  # (n+m,)\n",
    "\n",
    "    # 4) Softmax to get normalized probabilities\n",
    "    norm_probs = torch.softmax(scores, dim=0)                        # (n+m,)\n",
    "\n",
    "    # 5) Sum up the negative-feature mass & compute logit\n",
    "    num_pos   = len(pos_features)\n",
    "    neg_prob  = norm_probs[num_pos:].sum()\n",
    "    neg_logit = torch.log(neg_prob) - torch.log(1 - neg_prob)\n",
    "\n",
    "    return norm_probs, scores, neg_logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "100591aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized probabilities: tensor([6.7517e-01, 1.5117e-01, 1.6862e-01, 9.6502e-07, 5.0486e-03],\n",
      "       device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "Unnormalized log-scores: tensor([ -1.1105,  -2.6071,  -2.4978, -14.5689,  -6.0064], device='mps:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "Negative-feature logit: tensor(-5.2836, device='mps:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ——— Usage Example ———\n",
    "prompt       = \"<|User|> What is 6 times 6?<|Assistant|><think>\\n\"\n",
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "norm_probs, scores, neg_logit = compute_feature_mode_metric(\n",
    "    model, prompt, pos_features, neg_features\n",
    ")\n",
    "\n",
    "print(\"Normalized probabilities:\", norm_probs)\n",
    "print(\"Unnormalized log-scores:\", scores)\n",
    "print(\"Negative-feature logit:\", neg_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a778d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ac_path in glob.glob(\"activity_container_pickles/*.pkl\")[20:]:\n",
    "    ac = load_pickle(ac_path)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b17cf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "string, tokens = extract_output(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3f02c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>If a pizza is cut into 8 equal slices and 3 slices are eaten, what fraction remains?<｜Assistant｜><think>\\nTo determine the fraction of the pizza that remains after eating 3 slices out of 8, I start by noting that the total number of slices is 8.\\n\\nNext, I calculate the fraction of the pizza that has been eaten by dividing the number of eaten slices by the total number of slices, which gives me 3/8.\\n\\nFinally, to find the fraction that remains, I subtract the eaten fraction from the whole, resulting in 1 minus 3/8, which equals 5/8.\\n</think>\\n\\nTo determine the fraction of the pizza that remains after eating 3 slices out of 8, follow these steps:\\n\\n1. **Total Slices:** The pizza is cut into **8 equal slices**.\\n\\n2. **Eaten Slices:** You eat **3 slices**.\\n\\n3. **Fraction Eaten:**\\n   \\\\[\\n   \\\\text{Fraction Eaten} = \\\\frac{\\\\text{Eaten Slices}}{\\\\text{Total Slices}} = \\\\frac{3}{8}\\n   \\\\]\\n\\n4. **Fraction Remaining:**\\n   \\\\[\\n   \\\\text{Fraction Remaining} = 1 - \\\\frac{\\\\text{Fraction Eaten}}{1} = 1 - \\\\frac{3}{8} = \\\\frac{8}{8} - \\\\frac{3}{8} = \\\\frac{5}{8}\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{\\\\dfrac{5}{8}}\\n\\\\]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daaee021",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 81.47 GB, other allocations: 130.70 MB, max allowed: 81.60 GB). Tried to allocate 64.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m pos_features = [[\u001b[32m1271\u001b[39m], [\u001b[32m1271\u001b[39m, \u001b[32m1505\u001b[39m], [\u001b[32m1271\u001b[39m, \u001b[32m8417\u001b[39m], [\u001b[32m334\u001b[39m, \u001b[32m37942\u001b[39m, \u001b[32m25\u001b[39m]]\n\u001b[32m      3\u001b[39m neg_features = [[\u001b[32m33413\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m norm_probs, scores, neg_logit = \u001b[43mcompute_feature_mode_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_features\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNormalized probabilities:\u001b[39m\u001b[33m\"\u001b[39m, norm_probs)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnnormalized log-scores:\u001b[39m\u001b[33m\"\u001b[39m, scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcompute_feature_mode_metric\u001b[39m\u001b[34m(model, prompt, pos_features, neg_features)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 3) Compute scores for all features\u001b[39;00m\n\u001b[32m     34\u001b[39m all_features = pos_features + neg_features\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m scores = torch.stack(\u001b[43m[\u001b[49m\u001b[43msequence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# (n+m,)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 4) Softmax to get normalized probabilities\u001b[39;00m\n\u001b[32m     38\u001b[39m norm_probs = torch.softmax(scores, dim=\u001b[32m0\u001b[39m)                        \u001b[38;5;66;03m# (n+m,)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 3) Compute scores for all features\u001b[39;00m\n\u001b[32m     34\u001b[39m all_features = pos_features + neg_features\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m scores = torch.stack([\u001b[43msequence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_features])  \u001b[38;5;66;03m# (n+m,)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 4) Softmax to get normalized probabilities\u001b[39;00m\n\u001b[32m     38\u001b[39m norm_probs = torch.softmax(scores, dim=\u001b[32m0\u001b[39m)                        \u001b[38;5;66;03m# (n+m,)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mcompute_feature_mode_metric.<locals>.sequence_score\u001b[39m\u001b[34m(feature)\u001b[39m\n\u001b[32m     22\u001b[39m total_log_prob = torch.tensor(\u001b[32m0.0\u001b[39m, device=device)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m feature:\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# run the model on the current context\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     logits, cache = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     last_logits = logits[:, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# [1, vocab_size]\u001b[39;00m\n\u001b[32m     27\u001b[39m     log_probs = torch.log_softmax(last_logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:694\u001b[39m, in \u001b[36mHookedTransformer.run_with_cache\u001b[39m\u001b[34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_cache\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m, *model_args, return_cache_object=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m    679\u001b[39m ) -> Tuple[\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    687\u001b[39m ]:\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[32m    689\u001b[39m \n\u001b[32m    690\u001b[39m \u001b[33;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     out, cache_dict = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[32m    698\u001b[39m         cache = ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim=\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/hook_points.py:569\u001b[39m, in \u001b[36mHookedRootModule.run_with_cache\u001b[39m\u001b[34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m cache_dict, fwd, bwd = \u001b[38;5;28mself\u001b[39m.get_caching_hooks(\n\u001b[32m    556\u001b[39m     names_filter,\n\u001b[32m    557\u001b[39m     incl_bwd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     pos_slice=pos_slice,\n\u001b[32m    561\u001b[39m )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(\n\u001b[32m    564\u001b[39m     fwd_hooks=fwd,\n\u001b[32m    565\u001b[39m     bwd_hooks=bwd,\n\u001b[32m    566\u001b[39m     reset_hooks_end=reset_hooks_end,\n\u001b[32m    567\u001b[39m     clear_contexts=clear_contexts,\n\u001b[32m    568\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[32m    571\u001b[39m         model_out.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:196\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    170\u001b[39m     query_input: Union[\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     position_bias: Optional[Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33m1 head_index pos kv_pos\u001b[39m\u001b[33m\"\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    188\u001b[39m ) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos d_model\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    189\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m    shortformer_pos_embed is only used if self.cfg.positional_embedding_type == \"shortformer\", else defaults to None and is irrelevant. See HookedTransformerConfig for more details\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03m    past_kv_cache_entry is an optional entry of past keys and values for this layer, only relevant if generating text. Defaults to None\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m    additive_attention_mask is an optional mask to add to the attention weights. Defaults to None.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m    attention_mask is the attention mask for padded tokens. Defaults to None.\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     q, k, v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_qkv_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[32m    200\u001b[39m         kv_cache_pos_offset = past_kv_cache_entry.past_keys.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/components/grouped_query_attention.py:127\u001b[39m, in \u001b[36mGroupedQueryAttention.calculate_qkv_matrices\u001b[39m\u001b[34m(self, query_input, key_input, value_input)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calculate the Q, K, and V matrices for grouped query attention.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03mThis function uses the unexpanded weights _W_K and _W_V to calculate K and V.\u001b[39;00m\n\u001b[32m    110\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03mA tuple containing the Q, K, and V matrices with the specified shapes.\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m attn_fn = (\n\u001b[32m    121\u001b[39m     complex_attn_linear\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_split_qkv_input \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_attn_in\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m simple_attn_linear\n\u001b[32m    124\u001b[39m )\n\u001b[32m    126\u001b[39m q = \u001b[38;5;28mself\u001b[39m.hook_q(\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43mattn_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mb_Q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[32m    130\u001b[39m k = \u001b[38;5;28mself\u001b[39m.hook_k(\n\u001b[32m    131\u001b[39m     attn_fn(key_input, \u001b[38;5;28mself\u001b[39m.W_K, \u001b[38;5;28mself\u001b[39m.b_K)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.ungroup_grouped_query_attention\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m attn_fn(key_input, \u001b[38;5;28mself\u001b[39m._W_K, \u001b[38;5;28mself\u001b[39m._b_K)\n\u001b[32m    134\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[32m    135\u001b[39m v = \u001b[38;5;28mself\u001b[39m.hook_v(\n\u001b[32m    136\u001b[39m     attn_fn(value_input, \u001b[38;5;28mself\u001b[39m.W_V, \u001b[38;5;28mself\u001b[39m.b_V)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.ungroup_grouped_query_attention\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m attn_fn(value_input, \u001b[38;5;28mself\u001b[39m._W_V, \u001b[38;5;28mself\u001b[39m._b_V)\n\u001b[32m    139\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/transformer_lens/utilities/attention.py:24\u001b[39m, in \u001b[36msimple_attn_linear\u001b[39m\u001b[34m(input, w, b)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.device != b.device:\n\u001b[32m     22\u001b[39m     b = b.to(\u001b[38;5;28minput\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m w = \u001b[43meinops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhead_index d_model d_head -> (head_index d_head) d_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m b_ = einops.rearrange(b, \u001b[33m\"\u001b[39m\u001b[33mhead_index d_head -> (head_index d_head)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, w, b_).reshape(\u001b[38;5;28minput\u001b[39m.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28minput\u001b[39m.shape[\u001b[32m1\u001b[39m], b.shape[\u001b[32m0\u001b[39m], b.shape[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/einops/einops.py:600\u001b[39m, in \u001b[36mrearrange\u001b[39m\u001b[34m(tensor, pattern, **axes_lengths)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, **axes_lengths: Size) -> Tensor:\n\u001b[32m    546\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \n\u001b[32m    599\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrearrange\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/nnsight/tracing/graph/proxy.py:336\u001b[39m, in \u001b[36mproxy_patch.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m found \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m found.node.graph.create(\n\u001b[32m    331\u001b[39m         fn,\n\u001b[32m    332\u001b[39m         *args,\n\u001b[32m    333\u001b[39m         **kwargs,\n\u001b[32m    334\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/einops/einops.py:532\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    530\u001b[39m     shape = backend.shape(tensor)\n\u001b[32m    531\u001b[39m     recipe = _prepare_transformation_recipe(pattern, reduction, axes_names=\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim=\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhashable_axes_lengths\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     message = \u001b[33m'\u001b[39m\u001b[33m Error while processing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m-reduction pattern \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m.format(reduction, pattern)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/nnsight/tracing/graph/proxy.py:336\u001b[39m, in \u001b[36mproxy_patch.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m found \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m found.node.graph.create(\n\u001b[32m    331\u001b[39m         fn,\n\u001b[32m    332\u001b[39m         *args,\n\u001b[32m    333\u001b[39m         **kwargs,\n\u001b[32m    334\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/einops/einops.py:251\u001b[39m, in \u001b[36m_apply_recipe\u001b[39m\u001b[34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[39m\n\u001b[32m    249\u001b[39m     tensor = backend.add_axes(tensor, n_axes=n_axes_w_added, pos2len=added_axes)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     tensor = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/interp/venv/lib/python3.11/site-packages/einops/_backends.py:93\u001b[39m, in \u001b[36mAbstractBackend.reshape\u001b[39m\u001b[34m(self, x, shape)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreshape\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, shape):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 81.47 GB, other allocations: 130.70 MB, max allowed: 81.60 GB). Tried to allocate 64.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "\n",
    "prompt       = \"<｜User｜>If a pizza is cut into 8 equal slices and 3 slices are eaten, what fraction remains?<｜Assistant｜><think>\\n\"\n",
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "norm_probs, scores, neg_logit = compute_feature_mode_metric(\n",
    "    model, prompt, pos_features, neg_features\n",
    ")\n",
    "\n",
    "print(\"Normalized probabilities:\", norm_probs)\n",
    "print(\"Unnormalized log-scores:\", scores)\n",
    "print(\"Negative-feature logit:\", neg_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt       = \"<｜User｜>If a pizza is cut into 8 equal slices and 3 slices are eaten, what fraction remains?<｜Assistant｜><think>\\nTo determine the fraction of the pizza that remains after eating 3 slices out of 8, I start by noting that the total number of slices is 8.\\n\\nNext, I calculate the fraction of the pizza that has been eaten by dividing the number of eaten slices by the total number of slices, which gives me 3/8.\\n\\nFinally, to find the fraction that remains, I subtract the eaten fraction from the whole, resulting in 1 minus 3/8, which equals 5/8.\\n</think>\\n\\n\"\n",
    "pos_features = [[1271], [1271, 1505], [1271, 8417], [334, 37942, 25]]\n",
    "neg_features = [[33413]]\n",
    "\n",
    "norm_probs, scores, neg_logit = compute_feature_mode_metric(\n",
    "    model, prompt, pos_features, neg_features\n",
    ")\n",
    "\n",
    "print(\"Normalized probabilities:\", norm_probs)\n",
    "print(\"Unnormalized log-scores:\", scores)\n",
    "print(\"Negative-feature logit:\", neg_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669f7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
