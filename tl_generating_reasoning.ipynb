{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fcd759",
   "metadata": {},
   "source": [
    "In this notebook we'll be using transformerlens and runpods to generate outputs \n",
    "\n",
    "I'll do this under head ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4b408",
   "metadata": {},
   "source": [
    "runpods questions \n",
    "\n",
    "do I have a credit card number to use for runpods?\n",
    "\n",
    "how do I make sure runpods start when I want them and stop when im finished?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cd6cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mech_interp_research/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "import hashlib\n",
    "import yaml \n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save object to a file\n",
    "def save_pickle(obj, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Load object from a file\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283ae590",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0221b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_path = 'meta-llama/Llama-3.1-8B'\n",
    "baseline_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdc8edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "baseline_model_hf = AutoModelForCausalLM.from_pretrained(baseline_model_path, torch_dtype=torch.bfloat16)\n",
    "baseline_model_tokenizer = AutoTokenizer.from_pretrained(baseline_model_path)\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    reference_model_path,\n",
    "    hf_model=baseline_model_hf,\n",
    "    tokenizer=baseline_model_tokenizer,\n",
    "    device=device,\n",
    "    move_to_device=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b48165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7696008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_head_ablation(head_idx):\n",
    "    def head_ablation_hook(\n",
    "        value,\n",
    "        hook\n",
    "    ):\n",
    "        # print(f\"Shape of the value tensor: {value.shape}\")\n",
    "        value[:, :, head_idx, :] = 0.\n",
    "        return value\n",
    "\n",
    "    return head_ablation_hook\n",
    "\n",
    "\n",
    "def generate_using_ablated_head(model, init_strings, ablate_heads=None, max_new_tokens=300, format_init_strings=True, save_as_you_go=None):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "    if format_init_strings:\n",
    "        init_strings=[f\"<｜User｜>{string}<｜Assistant｜><think>\\n\" for string in init_strings]\n",
    "    \n",
    "    # apply ablations\n",
    "    model.reset_hooks()\n",
    "    if ablate_heads is not None:\n",
    "        for layer_idx, head_idx in ablate_heads:\n",
    "            cur_head_ablation = produce_head_ablation(head_idx)\n",
    "            model.add_hook(utils.get_act_name(\"z\", layer_idx), cur_head_ablation)\n",
    "\n",
    "        print(\"ablate heads:\")\n",
    "        for head in ablate_heads:\n",
    "            print(ablate_heads)\n",
    "    \n",
    "    if save_as_you_go is not None:\n",
    "        assert isinstance(save_as_you_go, str)\n",
    "        if '.pkl' != save_as_you_go[-4:]:\n",
    "            save_as_you_go = save_as_you_go+\".pkl\"\n",
    "\n",
    "    # now generate \n",
    "    # these are the default settings according \n",
    "    # https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
    "    temp=0.6 \n",
    "    top_p=0.95 \n",
    "\n",
    "    all_generated_outs = []\n",
    "    for init_string in init_strings:\n",
    "        test_tokens = model.to_tokens(init_string)\n",
    "        generated_outputs = model.generate(test_tokens, max_new_tokens=max_new_tokens, temperature=temp, top_p=top_p)\n",
    "\n",
    "        all_generated_outs.append(generated_outputs)\n",
    "\n",
    "        if save_as_you_go is not None:\n",
    "            save_pickle(all_generated_outs, save_as_you_go)\n",
    "    \n",
    "    return all_generated_outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99fc61",
   "metadata": {},
   "source": [
    "load the collected think dict and ablate groups of important think heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551fe146",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('collected_think_dict.yaml', 'r') as file:\n",
    "    think_dict = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ba48f",
   "metadata": {},
   "source": [
    "# ranking head importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6a49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_str, mean_think_stop_think_ratios = zip(\n",
    "    *sorted(\n",
    "        [(k, think_dict[k]['mean_think_stop_think_ratio']) for k in think_dict],\n",
    "        key = lambda x: x[-1],\n",
    "        reverse=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95a61082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_22 8.45154857635498\n",
      "3_1 6.1095075607299805\n",
      "0_21 4.9486775398254395\n",
      "4_21 4.679234027862549\n",
      "8_18 4.370467185974121\n",
      "1_23 4.120393753051758\n",
      "1_12 3.7947165966033936\n",
      "1_22 3.7929306030273438\n",
      "16_20 3.6640069484710693\n",
      "7_21 3.477773666381836\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(head_str[i], mean_think_stop_think_ratios[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a5c64",
   "metadata": {},
   "source": [
    "We'll take the top 4 important heads and ablate them one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d9ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_4_targets = [tuple([int(x) for x in h.split('_')]) for h in head_str[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "569a63be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 22), (3, 1), (0, 21), (4, 21)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_4_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d557d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_prompts(file_path=\"reasoning-prompts.md\"):\n",
    "    \"\"\"\n",
    "    Read prompts from a text file and return them as a list of strings.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file containing reasoning prompts\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of strings, each containing a reasoning prompt\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Skip empty lines, headers, and category labels\n",
    "                line = line.strip()\n",
    "                if (line and \n",
    "                    not line.startswith('#') and \n",
    "                    not line.startswith('##') and\n",
    "                    not line == \"\"):\n",
    "                    \n",
    "                    # Extract the prompt text by removing the number and period\n",
    "                    parts = line.split('. ', 1)\n",
    "                    if len(parts) > 1 and parts[0].isdigit():\n",
    "                        prompt = parts[1]\n",
    "                    else:\n",
    "                        prompt = line\n",
    "                        \n",
    "                    prompts.append(prompt)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading prompts: {e}\")\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65208697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_prompts = read_prompts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "801086e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablate heads:\n",
      "[(0, 22)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 233/300 [00:17<00:05, 13.27it/s]\n",
      " 58%|█████▊    | 174/300 [00:12<00:09, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 52%|█████▏    | 156/300 [00:11<00:10, 13.57it/s]\n",
      " 54%|█████▍    | 163/300 [00:12<00:10, 13.49it/s]\n",
      " 60%|██████    | 181/300 [00:13<00:08, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      " 79%|███████▊  | 236/300 [00:17<00:04, 13.57it/s]\n",
      " 77%|███████▋  | 230/300 [00:16<00:05, 13.56it/s]\n",
      " 79%|███████▊  | 236/300 [00:17<00:04, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 81%|████████  | 243/300 [00:17<00:04, 13.57it/s]\n",
      " 82%|████████▏ | 247/300 [00:18<00:03, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 65%|██████▌   | 195/300 [00:14<00:07, 13.57it/s]\n",
      " 72%|███████▏  | 217/300 [00:16<00:06, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 82%|████████▏ | 245/300 [00:18<00:04, 13.51it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 83%|████████▎ | 249/300 [00:18<00:03, 13.53it/s]\n",
      " 82%|████████▏ | 246/300 [00:18<00:03, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.58it/s]\n",
      " 72%|███████▏  | 215/300 [00:15<00:06, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 82%|████████▏ | 245/300 [00:18<00:04, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 94%|█████████▎| 281/300 [00:20<00:01, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 59%|█████▉    | 177/300 [00:13<00:09, 13.58it/s]\n",
      " 80%|███████▉  | 239/300 [00:17<00:04, 13.54it/s]\n",
      " 71%|███████   | 212/300 [00:15<00:06, 13.60it/s]\n",
      " 74%|███████▎  | 221/300 [00:16<00:05, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 88%|████████▊ | 265/300 [00:19<00:02, 13.57it/s]\n",
      " 84%|████████▍ | 253/300 [00:18<00:03, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 63%|██████▎   | 190/300 [00:14<00:08, 13.54it/s]\n",
      " 95%|█████████▍| 284/300 [00:20<00:01, 13.54it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 66%|██████▌   | 197/300 [00:14<00:07, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.57it/s]\n",
      " 95%|█████████▌| 286/300 [00:21<00:01, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 87%|████████▋ | 262/300 [00:19<00:02, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablate heads:\n",
      "[(0, 22), (3, 1)]\n",
      "[(0, 22), (3, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 280/300 [00:20<00:01, 13.59it/s]\n",
      " 62%|██████▏   | 185/300 [00:13<00:08, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 55%|█████▍    | 164/300 [00:12<00:10, 13.57it/s]\n",
      " 89%|████████▉ | 268/300 [00:19<00:02, 13.56it/s]\n",
      " 63%|██████▎   | 190/300 [00:13<00:08, 13.59it/s]\n",
      " 91%|█████████ | 273/300 [00:20<00:01, 13.57it/s]\n",
      " 90%|█████████ | 271/300 [00:19<00:02, 13.60it/s]\n",
      " 80%|████████  | 240/300 [00:17<00:04, 13.58it/s]\n",
      " 77%|███████▋  | 230/300 [00:16<00:05, 13.58it/s]\n",
      " 88%|████████▊ | 265/300 [00:19<00:02, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 75%|███████▌  | 225/300 [00:16<00:05, 13.59it/s]\n",
      " 87%|████████▋ | 260/300 [00:19<00:02, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.63it/s]\n",
      " 58%|█████▊    | 174/300 [00:12<00:09, 13.64it/s]\n",
      " 83%|████████▎ | 248/300 [00:18<00:03, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      " 81%|████████  | 243/300 [00:17<00:04, 13.57it/s]\n",
      " 59%|█████▉    | 177/300 [00:13<00:09, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 87%|████████▋ | 261/300 [00:19<00:02, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 66%|██████▋   | 199/300 [00:14<00:07, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 63%|██████▎   | 188/300 [00:13<00:08, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      " 95%|█████████▍| 284/300 [00:20<00:01, 13.56it/s]\n",
      " 95%|█████████▌| 286/300 [00:21<00:01, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 69%|██████▉   | 207/300 [00:15<00:06, 13.58it/s]\n",
      " 90%|█████████ | 271/300 [00:19<00:02, 13.55it/s]\n",
      " 73%|███████▎  | 218/300 [00:16<00:06, 13.58it/s]\n",
      " 87%|████████▋ | 260/300 [00:19<00:02, 13.55it/s]\n",
      " 89%|████████▉ | 268/300 [00:19<00:02, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 99%|█████████▉| 298/300 [00:21<00:00, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 51%|█████▏    | 154/300 [00:11<00:10, 13.57it/s]\n",
      " 74%|███████▎  | 221/300 [00:16<00:05, 13.56it/s]\n",
      " 90%|█████████ | 271/300 [00:20<00:02, 13.49it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.46it/s]\n",
      " 63%|██████▎   | 190/300 [00:14<00:08, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.56it/s]\n",
      " 78%|███████▊  | 233/300 [00:17<00:04, 13.54it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablate heads:\n",
      "[(0, 22), (3, 1), (0, 21)]\n",
      "[(0, 22), (3, 1), (0, 21)]\n",
      "[(0, 22), (3, 1), (0, 21)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 252/300 [00:18<00:03, 13.55it/s]\n",
      " 62%|██████▏   | 187/300 [00:13<00:08, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 61%|██████    | 183/300 [00:13<00:08, 13.57it/s]\n",
      " 72%|███████▏  | 215/300 [00:15<00:06, 13.55it/s]\n",
      " 54%|█████▍    | 162/300 [00:11<00:10, 13.57it/s]\n",
      " 82%|████████▏ | 245/300 [00:18<00:04, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 83%|████████▎ | 248/300 [00:18<00:03, 13.59it/s]\n",
      " 93%|█████████▎| 279/300 [00:20<00:01, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 72%|███████▏  | 215/300 [00:15<00:06, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 66%|██████▌   | 198/300 [00:14<00:07, 13.59it/s]\n",
      " 89%|████████▊ | 266/300 [00:19<00:02, 13.57it/s]\n",
      " 95%|█████████▍| 284/300 [00:20<00:01, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 82%|████████▏ | 246/300 [00:18<00:03, 13.58it/s]\n",
      " 50%|████▉     | 149/300 [00:10<00:11, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 75%|███████▌  | 226/300 [00:16<00:05, 13.59it/s]\n",
      "100%|██████████| 300/300 [00:21<00:00, 13.68it/s]\n",
      "100%|██████████| 300/300 [00:21<00:00, 13.67it/s]\n",
      " 70%|███████   | 210/300 [00:15<00:06, 13.66it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      "100%|██████████| 300/300 [00:21<00:00, 13.65it/s]\n",
      " 76%|███████▋  | 229/300 [00:16<00:05, 13.62it/s]\n",
      "100%|██████████| 300/300 [00:21<00:00, 13.64it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 95%|█████████▍| 284/300 [00:20<00:01, 13.55it/s]\n",
      " 91%|█████████ | 273/300 [00:20<00:01, 13.57it/s]\n",
      " 69%|██████▊   | 206/300 [00:15<00:06, 13.56it/s]\n",
      " 84%|████████▍ | 252/300 [00:18<00:03, 13.58it/s]\n",
      " 51%|█████     | 153/300 [00:11<00:10, 13.59it/s]\n",
      " 85%|████████▌ | 256/300 [00:18<00:03, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      " 89%|████████▉ | 267/300 [00:19<00:02, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 54%|█████▍    | 162/300 [00:11<00:10, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.59it/s]\n",
      " 67%|██████▋   | 201/300 [00:14<00:07, 13.57it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 66%|██████▌   | 197/300 [00:14<00:07, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablate heads:\n",
      "[(0, 22), (3, 1), (0, 21), (4, 21)]\n",
      "[(0, 22), (3, 1), (0, 21), (4, 21)]\n",
      "[(0, 22), (3, 1), (0, 21), (4, 21)]\n",
      "[(0, 22), (3, 1), (0, 21), (4, 21)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:21<00:00, 13.64it/s]\n",
      " 57%|█████▋    | 170/300 [00:12<00:09, 13.59it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 51%|█████     | 153/300 [00:11<00:10, 13.60it/s]\n",
      " 74%|███████▍  | 222/300 [00:16<00:05, 13.58it/s]\n",
      " 60%|██████    | 180/300 [00:13<00:08, 13.59it/s]\n",
      " 97%|█████████▋| 292/300 [00:21<00:00, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.61it/s]\n",
      " 99%|█████████▉| 298/300 [00:21<00:00, 13.58it/s]\n",
      " 81%|████████  | 242/300 [00:17<00:04, 13.58it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.60it/s]\n",
      " 99%|█████████▉| 298/300 [00:21<00:00, 13.57it/s]\n",
      " 76%|███████▋  | 229/300 [00:16<00:05, 13.55it/s]\n",
      " 88%|████████▊ | 264/300 [00:19<00:02, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.54it/s]\n",
      " 61%|██████    | 182/300 [00:13<00:08, 13.51it/s]\n",
      " 83%|████████▎ | 250/300 [00:18<00:03, 13.52it/s]\n",
      " 92%|█████████▏| 276/300 [00:20<00:01, 13.52it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.51it/s]\n",
      " 77%|███████▋  | 232/300 [00:17<00:05, 13.54it/s]\n",
      " 96%|█████████▌| 288/300 [00:21<00:00, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.56it/s]\n",
      " 75%|███████▍  | 224/300 [00:16<00:05, 13.51it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.55it/s]\n",
      " 65%|██████▌   | 195/300 [00:14<00:07, 13.52it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      " 62%|██████▏   | 186/300 [00:13<00:08, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.54it/s]\n",
      " 85%|████████▍ | 254/300 [00:18<00:03, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      " 65%|██████▌   | 195/300 [00:14<00:07, 13.53it/s]\n",
      " 73%|███████▎  | 220/300 [00:16<00:05, 13.50it/s]\n",
      " 51%|█████▏    | 154/300 [00:11<00:10, 13.52it/s]\n",
      " 95%|█████████▌| 286/300 [00:21<00:01, 13.49it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.57it/s]\n",
      " 76%|███████▋  | 229/300 [00:16<00:05, 13.49it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.55it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      " 57%|█████▋    | 170/300 [00:12<00:09, 13.43it/s]\n",
      " 82%|████████▏ | 247/300 [00:18<00:03, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.52it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.51it/s]\n",
      " 63%|██████▎   | 190/300 [00:14<00:08, 13.56it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.53it/s]\n",
      "100%|█████████▉| 299/300 [00:22<00:00, 13.50it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.54it/s]\n",
      " 90%|█████████ | 271/300 [00:20<00:02, 13.54it/s]\n"
     ]
    }
   ],
   "source": [
    "collected_outputs = {}\n",
    "\n",
    "for i in range(4):\n",
    "    save_string = f\"top_{i+1}_heads_ablated.pkl\"\n",
    "\n",
    "    generated_outputs = generate_using_ablated_head(\n",
    "        model, \n",
    "        init_strings=sampled_prompts, \n",
    "        ablate_heads=top_4_targets[:i+1], \n",
    "        max_new_tokens=300, \n",
    "        format_init_strings=True, \n",
    "        save_as_you_go=save_string\n",
    "    )\n",
    "\n",
    "    collected_outputs[save_string]=generated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80453d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:22<00:00, 13.58it/s]\n",
      " 57%|█████▋    | 170/300 [00:12<00:09, 13.51it/s]\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.55it/s]\n",
      " 61%|██████    | 182/300 [00:13<00:08, 13.55it/s]\n",
      " 67%|██████▋   | 202/300 [00:14<00:07, 13.53it/s]\n",
      " 52%|█████▏    | 155/300 [00:11<00:10, 13.54it/s]\n",
      " 10%|▉         | 29/300 [00:02<00:20, 13.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_using_ablated_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampled_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mablate_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformat_init_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_as_you_go\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_heads_ablated.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mgenerate_using_ablated_head\u001b[0;34m(model, init_strings, ablate_heads, max_new_tokens, format_init_strings, save_as_you_go)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m init_string \u001b[38;5;129;01min\u001b[39;00m init_strings:\n\u001b[1;32m     44\u001b[0m     test_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_tokens(init_string)\n\u001b[0;32m---> 45\u001b[0m     generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     all_generated_outs\u001b[38;5;241m.\u001b[39mappend(generated_outputs)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_as_you_go \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:2251\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_past_kv_cache:\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;66;03m# We just take the final tokens, as a [batch, 1] tensor\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2251\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_at_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_at_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2261\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m   2262\u001b[0m             residual,\n\u001b[1;32m   2263\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             shortformer_pos_embed\u001b[38;5;241m=\u001b[39mshortformer_pos_embed,\n\u001b[1;32m   2269\u001b[0m         )\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py:175\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n\u001b[0;32m--> 175\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_attn_out\u001b[49m(attn_out)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resid_pre\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m attn_out\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m    178\u001b[0m     resid_pre \u001b[38;5;241m=\u001b[39m resid_pre\u001b[38;5;241m.\u001b[39mto(attn_out\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/mech_interp_research/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generated_outputs = generate_using_ablated_head(\n",
    "    model, \n",
    "    init_strings=sampled_prompts, \n",
    "    ablate_heads=None, \n",
    "    max_new_tokens=300, \n",
    "    format_init_strings=True, \n",
    "    save_as_you_go=\"no_heads_ablated.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c401859",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(\"Hey whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  19182,  41209,    709,     30]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d9414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:29<00:00, 13.60it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.reset_hooks()\n",
    "original_logits = model(tokens, return_type=\"logits\")\n",
    "\n",
    "test_tokens = model.to_tokens([\"Hey whats up?\"])\n",
    "\n",
    "# try generating \n",
    "outs = model.generate(test_tokens, max_new_tokens=400, temperature=0.6, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3827a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<｜begin▁of▁sentence｜>Hey whats up? So I just saw this video where someone was trying to make a cake from scratch, but it didn't turn out so well. It was kinda funny but also kind of sad because the person put so much effort into it. I wonder why it didn't work out. Maybe they didn't measure the ingredients properly? Or did they forget to add baking powder? I mean, I've had my fair share of baking failures before, so I can relate. Let me think about how to approach this.\\n\\nFirst off, when I try baking, I usually just throw in whatever I have, but that's probably why my cakes sometimes end up too dense or too dry. Maybe I should double-check the recipes next time. But wait, the person in the video was following a recipe, right? So maybe the issue was with the measurements. Did they use the right amounts of flour, sugar, butter, etc.? It's easy to get confused with cup measurements versus grams or milliliters, especially if you're not used to it.\\n\\nAlso, could it be the oven temperature? I remember once I left the oven running a little too long, and the cake got too brown on top but was still gooey inside. That was not good. Maybe they left it in too long or the oven wasn't preheated properly. Or perhaps the baking pan wasn't the right size. If it's too big, the cake might not cook evenly, leading to a dense center. That could be it.\\n\\nAnother thing is the ingredients themselves. Maybe they used expired baking powder or the wrong type of flour. I once used all-purpose flour and the cake was really dry. Then I tried using self-rising flour, and it was much better. So maybe the type of flour matters a lot. Or if they used oil instead of butter or vice versa, that could change the texture.\\n\\nI should also consider the mixing method. Did they mix the batter properly? If you don't mix the dry and\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b569a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.0.attn.hook_z'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_act_name(\"z\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([1, 5, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.add_hook(utils.get_act_name(\"z\", layer_to_ablate), head_ablation_hook)\n",
    "\n",
    "\n",
    "modified_logits = model(tokens, return_type=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd83c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2196,  2.1443,  4.9758,  ..., -1.1626, -1.1626, -1.1626],\n",
       "         [12.2630,  1.2126,  0.5368,  ..., -3.3336, -3.3335, -3.3338],\n",
       "         [ 5.2130,  2.5234, -0.0555,  ..., -4.4290, -4.4293, -4.4291],\n",
       "         [11.5725,  3.8210,  1.2050,  ..., -5.2874, -5.2878, -5.2877],\n",
       "         [ 5.1244, -3.3234,  1.2445,  ..., -4.2034, -4.2034, -4.2036]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1601,  2.0860,  4.8631,  ..., -1.1549, -1.1549, -1.1549],\n",
       "         [12.2188,  1.1821,  0.5339,  ..., -3.3060, -3.3059, -3.3062],\n",
       "         [ 5.2511,  2.6107, -0.0394,  ..., -4.4606, -4.4609, -4.4606],\n",
       "         [11.5757,  3.8677,  1.2013,  ..., -5.2869, -5.2873, -5.2872],\n",
       "         [ 5.1116, -3.3044,  1.2272,  ..., -4.2133, -4.2133, -4.2136]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487148f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "original_logits2 = model(tokens, return_type=\"logits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1601,  2.0860,  4.8631,  ..., -1.1549, -1.1549, -1.1549],\n",
       "         [12.2188,  1.1821,  0.5339,  ..., -3.3060, -3.3059, -3.3062],\n",
       "         [ 5.2511,  2.6107, -0.0394,  ..., -4.4606, -4.4609, -4.4606],\n",
       "         [11.5757,  3.8677,  1.2013,  ..., -5.2869, -5.2873, -5.2872],\n",
       "         [ 5.1116, -3.3044,  1.2272,  ..., -4.2133, -4.2133, -4.2136]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e097c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
