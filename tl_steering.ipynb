{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1c9737",
   "metadata": {},
   "source": [
    "# trying to steer using transformerlens \n",
    "\n",
    "april 26 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebc9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mech_interp_research/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "import hashlib\n",
    "import yaml \n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0f24bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7429c47aac80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25761ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "\n",
    "reference_model_path = 'meta-llama/Llama-3.1-8B'\n",
    "baseline_model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "baseline_model_hf = AutoModelForCausalLM.from_pretrained(baseline_model_path, torch_dtype=torch.bfloat16)\n",
    "baseline_model_tokenizer = AutoTokenizer.from_pretrained(baseline_model_path)\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    reference_model_path,\n",
    "    hf_model=baseline_model_hf,\n",
    "    tokenizer=baseline_model_tokenizer,\n",
    "    device=device,\n",
    "    move_to_device=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f0780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vectors = np.load('layer_analyses/combined_delta_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f8ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_layer_resid_pre_steering_hook(steering_vector, scale=1.0):\n",
    "    def resid_pre_hook(\n",
    "        value,\n",
    "        hook\n",
    "    ):\n",
    "        value[:, -1, :] += steering_vector.to(value)*scale\n",
    "        return value\n",
    "\n",
    "    return resid_pre_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef711eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_steering_vectors(\n",
    "    model,\n",
    "    target_layer_indices,\n",
    "    weight_vectors,\n",
    "    scale=1.0,\n",
    "):\n",
    "    \"\"\" \n",
    "    Adds steering vectors to the model at the specified layer indices.\n",
    "    add only to resid_pre \n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The model to modify.\n",
    "        target_layer_indices (list of int): The layer indices to add steering vectors to.\n",
    "        weight_vectors (np.ndarray): The steering vectors to add.\n",
    "        scale (float): The scaling factor for the steering vectors.\n",
    "    \"\"\"\n",
    "    model.reset_hooks()\n",
    "    for block_idx, block in enumerate(model.blocks):\n",
    "        if block_idx in target_layer_indices:\n",
    "            # Get the steering vector for this block\n",
    "            steering_vector = weight_vectors[block_idx]\n",
    "            # Create the hook function\n",
    "            resid_pre_hook = produce_layer_resid_pre_steering_hook(steering_vector, scale)\n",
    "            # Add the hook to the model\n",
    "            model.add_hook(\n",
    "                utils.get_act_name('resid_pre', block_idx),\n",
    "                resid_pre_hook\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052b0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = [None]+[-torch.tensor(w[None, :], device='cuda:0') for w in weight_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431d784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vectors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb858159",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_steering_vectors(\n",
    "    model,\n",
    "    target_layer_indices=[1, 2, 3, 4],\n",
    "    weight_vectors=steering_vectors,\n",
    "    scale=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b27000",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"What's the fifth prime?\"},\n",
    "]\n",
    "\n",
    "prompt = model.tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89218133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 173/256 [00:12<00:06, 13.39it/s]\n"
     ]
    }
   ],
   "source": [
    "test_output = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08f3d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜User｜>What's the fifth prime?<｜Assistant｜><think>\n",
      "To determine the fifth prime number, I'll start by listing the prime numbers in order.\n",
      "\n",
      "First, I know that 2 is the smallest and only even prime number.\n",
      "\n",
      "Next, 3 is the second prime number.\n",
      "\n",
      "Then, 5 comes after 3 as the third prime.\n",
      "\n",
      "Following that, 7 is the fourth prime number.\n",
      "\n",
      "Finally, 11 is the fifth prime number.\n",
      "</think>\n",
      "\n",
      "To find the **fifth prime number**, let's list the prime numbers in order until we reach the fifth one.\n",
      "\n",
      "1. **2** - The first prime number.\n",
      "2. **3** - The second prime number.\n",
      "3. **5** - The third prime number.\n",
      "4. **7** - The fourth prime number.\n",
      "5. **11** - The fifth prime number.\n",
      "\n",
      "So, the fifth prime number is \\(\\boxed{11}\\).\n"
     ]
    }
   ],
   "source": [
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd7a6d",
   "metadata": {},
   "source": [
    "# Experiments \n",
    "\n",
    "**initial model**\n",
    "there exist heads which toggle thinking early on \n",
    "\n",
    "these signals \n",
    "\n",
    "\n",
    "\n",
    "1) which layer is most important? \n",
    "\n",
    "2) try steering with the OV circuit of head (0, 22) applied to <think> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904bf7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 128, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W_O.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3129dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_o = model.W_O[0, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54ddd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_steering_vector = model.W_E[128013]@model.W_V[0, 22]@model.W_O[0, 22]\n",
    "\n",
    "new_steering_vector /= torch.linalg.norm(new_steering_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1a72399",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_steering_vectors = [None]*32 \n",
    "\n",
    "new_steering_vectors = [new_steering_vector.reshape((1, 4096)) for _ in range(32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4613e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0001], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0995a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_steering_vectors(\n",
    "    model,\n",
    "    target_layer_indices=[0],\n",
    "    weight_vectors=new_steering_vectors,\n",
    "    scale=-.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1833d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:18<00:00, 13.66it/s]\n"
     ]
    }
   ],
   "source": [
    "test_output = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c4a4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜User｜>What's the fifth prime?<｜Assistant｜><think>\n",
      "Alright, so I need to figure out what the fifth prime number is. Hmm, primes are numbers greater than 1 that have no positive divisors other than 1 and themselves. Let me start by recalling the order of prime numbers. I think the first few primes are 2, 3, 5, 7, and then... hmm, is it 11? Wait, let me make sure I'm not making a mistake here. \n",
      "\n",
      "Okay, starting from the smallest prime, which is 2. Yeah, 2 is definitely prime because it's only divisible by1 and itself. Then comes3. Is3 prime? Yeah, because it's not divisible by2 or any other number except1 and3. Next is4, but wait,4 is divisible by2, so that's not prime. Then5, which is prime because it's only divisible by1 and5. \n",
      "\n",
      "Now, moving on,6 is divisible by2 and3, so not prime.7 comes next. Is7 prime? Let me check. 7 divided by2? No, it's 3.5? No, because7 divided by5 is1.2? No, same as above. So, yeah,7 is prime. \n",
      "\n",
      "So\n"
     ]
    }
   ],
   "source": [
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf3f7b",
   "metadata": {},
   "source": [
    "Q: can we use one of the lda directions to steer think vs nonthink  \n",
    "\n",
    "lda is think vs nonthink in later layers \n",
    "\n",
    "and less significant components control turning this on early on \n",
    "---------\n",
    "how does stop think get triggered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35110f71",
   "metadata": {},
   "source": [
    "difference in mean vector vs lda direction \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c185f48",
   "metadata": {},
   "source": [
    "# ideas \n",
    "\n",
    " - when we injected ov(0, 22).think at layer 0, the network thought, tried to end thinking, and did so repeatedly  \n",
    "\n",
    "what if we look at the logit of <\\think> after adding in different ov.think vectors from different heads \n",
    "\n",
    "which one contributed something that's most like \"think\"\n",
    "\n",
    "- by what mechanism does the model try to activate <\\think>? \n",
    "\n",
    "once its finished thinking, it tries to stop \n",
    "\n",
    "what do the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3480e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "<\\think>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
